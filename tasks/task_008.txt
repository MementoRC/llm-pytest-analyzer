# Task ID: 8
# Title: Implement Core Analysis Tool: analyze_pytest_output
# Status: pending
# Dependencies: 6, 7
# Priority: high
# Description: Implement the MCP tool for analyzing pytest output files and generating fix suggestions.
# Details:
In `src/pytest_analyzer/mcp/tools/analysis.py`, implement:
1. `analyze_pytest_output` tool function
2. Input validation using schemas
3. Integration with MCPAnalyzerFacade
4. Error handling and logging
5. Performance monitoring

Implementation should:
- Accept file path to pytest output (JSON, XML, or text)
- Detect file format automatically
- Validate file existence and size
- Return structured list of fix suggestions with confidence scores
- Include execution time metrics

Example implementation:
```python
from pytest_analyzer.mcp.facade import MCPAnalyzerFacade
from pytest_analyzer.mcp.schemas import AnalyzePytestOutputRequest, AnalyzePytestOutputResponse
import time

async def analyze_pytest_output(request: AnalyzePytestOutputRequest, facade: MCPAnalyzerFacade) -> AnalyzePytestOutputResponse:
    start_time = time.time()

    # Validate file size
    file_size = os.path.getsize(request.file_path) / (1024 * 1024)  # Size in MB
    if file_size > 10:  # 10MB limit
        raise ValueError(f"File too large: {file_size:.2f}MB (max 10MB)")

    # Delegate to facade
    result = await facade.analyze_pytest_output(request)

    # Add execution time
    execution_time_ms = int((time.time() - start_time) * 1000)
    result.execution_time_ms = execution_time_ms

    return result
```

Register this tool with the MCP server during initialization.

# Test Strategy:
Unit test with mock facade. Test with various pytest output formats (JSON, XML, text). Verify file validation logic works correctly. Test with large files to check size limits. Measure performance with realistic test data. Integration test with actual facade implementation.
