{
  "tasks": [
    {
      "id": 1,
      "title": "Create Project Architecture Document",
      "description": "Create a comprehensive architecture document outlining the new structure, component relationships, and interfaces based on the refactoring requirements.",
      "details": "The architecture document should include:\n1. Component diagram showing the relationships between Extractors, Analyzers, Suggesters, Appliers, and LLM Services\n2. Interface definitions for each component using Python's Protocol type\n3. Data flow diagrams\n4. Dependency injection approach\n5. Error handling strategy\n6. State machine design\n7. Both synchronous and asynchronous API patterns\n\nThe document should serve as a reference for the entire refactoring process and ensure all team members have a clear understanding of the target architecture.",
      "testStrategy": "Review the document with stakeholders to ensure it addresses all requirements in the PRD. Validate that the proposed architecture supports all current functionality while enabling the improvements specified in the project goals.",
      "priority": "high",
      "dependencies": [],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 2,
      "title": "Implement Core Protocol Interfaces",
      "description": "Define Protocol interfaces for all major components to establish clear contracts between system parts.",
      "details": "Create Protocol classes for each major component type:\n\n```python\nfrom typing import Protocol, List, Dict, Any, Optional\n\nclass Extractor(Protocol):\n    def extract(self, test_results: Any) -> Dict[str, Any]: ...\n\nclass Analyzer(Protocol):\n    def analyze(self, extraction_results: Dict[str, Any]) -> Dict[str, Any]: ...\n\nclass Suggester(Protocol):\n    def suggest(self, analysis_results: Dict[str, Any]) -> List[Dict[str, Any]]: ...\n\nclass Applier(Protocol):\n    def apply(self, suggestions: List[Dict[str, Any]], target_files: List[str]) -> bool: ...\n\nclass LLMService(Protocol):\n    def generate(self, prompt: str, **kwargs) -> str: ...\n    async def generate_async(self, prompt: str, **kwargs) -> str: ...\n```\n\nEnsure each Protocol has proper type hints and docstrings explaining the expected behavior and contract.",
      "testStrategy": "Create test cases that verify implementations conform to the Protocol interfaces. Use mypy or similar static type checking to validate interface compliance.",
      "priority": "high",
      "dependencies": [
        1
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 3,
      "title": "Implement Dependency Injection Container",
      "description": "Create a dependency injection container to manage component dependencies and facilitate testing.",
      "details": "Implement a DI container that will manage the creation and lifecycle of all components:\n\n```python\nfrom typing import Dict, Type, Any, TypeVar, Generic, cast\n\nT = TypeVar('T')\n\nclass DIContainer:\n    def __init__(self):\n        self._services: Dict[Type, Any] = {}\n        self._factories: Dict[Type, callable] = {}\n    \n    def register(self, interface_type: Type[T], implementation: T) -> None:\n        self._services[interface_type] = implementation\n    \n    def register_factory(self, interface_type: Type[T], factory: callable) -> None:\n        self._factories[interface_type] = factory\n    \n    def resolve(self, interface_type: Type[T]) -> T:\n        if interface_type in self._services:\n            return cast(T, self._services[interface_type])\n        \n        if interface_type in self._factories:\n            implementation = self._factories[interface_type]()\n            self._services[interface_type] = implementation\n            return cast(T, implementation)\n            \n        raise KeyError(f\"No registration found for {interface_type.__name__}\")\n```\n\nProvide helper methods for registering and resolving dependencies. Include documentation on how to use the container in different contexts (production, testing).",
      "testStrategy": "Write unit tests that verify the container can register and resolve dependencies correctly. Test both direct registration and factory-based registration. Verify proper error handling for missing dependencies.",
      "priority": "high",
      "dependencies": [
        2
      ],
      "status": "done",
      "subtasks": [
        {
          "id": 1,
          "title": "Implement Core Container Structure",
          "description": "Create the foundational structure for the dependency injection container with basic registration and resolution capabilities.",
          "dependencies": [],
          "details": "Implement the Container class with internal storage for registered dependencies. Include methods for basic registration and resolution. Implement proper type annotations using generics. Add error handling for missing dependencies. Write unit tests for basic container functionality including registration, resolution, and error cases.",
          "status": "done"
        },
        {
          "id": 2,
          "title": "Develop Advanced Registration Mechanisms",
          "description": "Extend the container with various registration options including singleton, transient, and factory registrations.",
          "dependencies": [
            1
          ],
          "details": "Implement singleton registration to ensure only one instance is created. Add transient registration for new instances on each resolution. Create factory registration for custom instance creation logic. Implement decorator-based registration for cleaner syntax. Write unit tests for each registration type, verifying correct lifecycle management.",
          "status": "done"
        },
        {
          "id": 3,
          "title": "Implement Dependency Resolution Logic",
          "description": "Create advanced resolution capabilities including automatic dependency injection and circular dependency detection.",
          "dependencies": [
            1,
            2
          ],
          "details": "Implement automatic constructor injection by analyzing parameter types. Add support for resolving nested dependencies. Implement circular dependency detection and prevention. Create mechanisms for optional dependencies. Write comprehensive tests for complex resolution scenarios including nested dependencies and edge cases.",
          "status": "done"
        },
        {
          "id": 4,
          "title": "Create Documentation and Usage Examples",
          "description": "Develop comprehensive documentation and example code demonstrating container usage patterns.",
          "dependencies": [
            1,
            2,
            3
          ],
          "details": "Write detailed API documentation with type information. Create usage examples for common scenarios. Document best practices and anti-patterns. Implement a sample application demonstrating real-world usage. Create integration tests that verify the examples work as documented.",
          "status": "done"
        }
      ]
    },
    {
      "id": 4,
      "title": "Implement LLM Service Interface and Base Implementation",
      "description": "Create a properly abstracted LLM service with both synchronous and asynchronous APIs.",
      "details": "Implement the LLM service with the following components:\n\n```python\nfrom typing import Dict, Any, Optional, Protocol\nimport aiohttp\nimport requests\nfrom contextlib import contextmanager\n\nclass LLMConfig(Protocol):\n    api_key: str\n    model_name: str\n    temperature: float\n    max_tokens: int\n    # Other configuration parameters\n\nclass LLMService:\n    def __init__(self, config: LLMConfig):\n        self.config = config\n        \n    @contextmanager\n    def _error_context(self, operation: str):\n        try:\n            yield\n        except Exception as e:\n            # Log the error with context\n            raise LLMServiceError(f\"Error during {operation}: {str(e)}\") from e\n    \n    def generate(self, prompt: str, **kwargs) -> str:\n        with self._error_context(\"synchronous generation\"):\n            # Implementation for synchronous API call\n            params = {**self.config.__dict__, **kwargs}\n            response = requests.post(\n                \"https://api.openai.com/v1/completions\",\n                headers={\"Authorization\": f\"Bearer {self.config.api_key}\"},\n                json={\"prompt\": prompt, **params}\n            )\n            response.raise_for_status()\n            return response.json()[\"choices\"][0][\"text\"]\n    \n    async def generate_async(self, prompt: str, **kwargs) -> str:\n        with self._error_context(\"asynchronous generation\"):\n            # Implementation for asynchronous API call\n            params = {**self.config.__dict__, **kwargs}\n            async with aiohttp.ClientSession() as session:\n                async with session.post(\n                    \"https://api.openai.com/v1/completions\",\n                    headers={\"Authorization\": f\"Bearer {self.config.api_key}\"},\n                    json={\"prompt\": prompt, **params}\n                ) as response:\n                    if response.status != 200:\n                        text = await response.text()\n                        raise LLMServiceError(f\"API error: {response.status}, {text}\")\n                    data = await response.json()\n                    return data[\"choices\"][0][\"text\"]\n\nclass LLMServiceError(Exception):\n    pass\n```\n\nEnsure the implementation is configurable, handles errors properly, and provides both synchronous and asynchronous APIs.",
      "testStrategy": "Create unit tests with mocked HTTP responses to test both successful and error scenarios. Test both synchronous and asynchronous APIs. Verify error handling and context managers work as expected.",
      "priority": "high",
      "dependencies": [
        2,
        3
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 5,
      "title": "Implement PromptBuilder Component",
      "description": "Create a dedicated component for constructing prompts to send to the LLM service.",
      "details": "Implement a PromptBuilder that handles template management and prompt construction:\n\n```python\nfrom string import Template\nfrom typing import Dict, Any, Optional\nimport os\nimport json\n\nclass PromptBuilder:\n    def __init__(self, template_dir: Optional[str] = None):\n        self.template_dir = template_dir or os.path.join(os.path.dirname(__file__), \"templates\")\n        self.templates: Dict[str, Template] = {}\n        self._load_templates()\n    \n    def _load_templates(self) -> None:\n        # Load templates from files\n        if os.path.exists(self.template_dir):\n            for filename in os.listdir(self.template_dir):\n                if filename.endswith(\".txt\"):\n                    template_name = os.path.splitext(filename)[0]\n                    with open(os.path.join(self.template_dir, filename), \"r\") as f:\n                        self.templates[template_name] = Template(f.read())\n    \n    def build_prompt(self, template_name: str, **kwargs) -> str:\n        if template_name not in self.templates:\n            raise ValueError(f\"Template '{template_name}' not found\")\n        \n        # Handle special cases like code formatting\n        for key, value in kwargs.items():\n            if isinstance(value, dict):\n                kwargs[key] = json.dumps(value, indent=2)\n        \n        return self.templates[template_name].substitute(**kwargs)\n    \n    def register_template(self, name: str, template_string: str) -> None:\n        self.templates[name] = Template(template_string)\n```\n\nCreate a set of default templates for common operations like test failure analysis, fix suggestion, etc. Store these in a templates directory.",
      "testStrategy": "Write unit tests that verify template loading, prompt building with various parameter types, and error handling for missing templates. Test with both file-based templates and programmatically registered templates.",
      "priority": "medium",
      "dependencies": [
        2,
        4
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 6,
      "title": "Implement ResponseParser Component",
      "description": "Create a dedicated component for parsing and validating responses from the LLM service.",
      "details": "Implement a ResponseParser that handles different response formats and validation:\n\n```python\nfrom typing import Dict, Any, List, Optional, TypeVar, Generic, Type, cast\nimport json\nfrom pydantic import BaseModel, ValidationError\n\nT = TypeVar('T', bound=BaseModel)\n\nclass ResponseParser(Generic[T]):\n    def __init__(self, model_class: Type[T]):\n        self.model_class = model_class\n    \n    def parse_json(self, response_text: str) -> T:\n        try:\n            # Extract JSON from response if needed\n            json_str = self._extract_json(response_text)\n            # Parse JSON\n            data = json.loads(json_str)\n            # Validate with Pydantic model\n            return self.model_class.parse_obj(data)\n        except json.JSONDecodeError as e:\n            raise ResponseParseError(f\"Invalid JSON in response: {str(e)}\") from e\n        except ValidationError as e:\n            raise ResponseParseError(f\"Response validation failed: {str(e)}\") from e\n    \n    def _extract_json(self, text: str) -> str:\n        # Handle cases where JSON is embedded in markdown or other text\n        # This is a simple implementation - might need to be more robust\n        if text.strip().startswith('{') and text.strip().endswith('}'): \n            return text\n        \n        # Try to find JSON between triple backticks\n        import re\n        json_match = re.search(r'```(?:json)?\\s*(.+?)\\s*```', text, re.DOTALL)\n        if json_match:\n            return json_match.group(1)\n            \n        # If no JSON found, raise error\n        raise ResponseParseError(\"Could not extract JSON from response\")\n\nclass ResponseParseError(Exception):\n    pass\n```\n\nCreate Pydantic models for different response types (analysis results, fix suggestions, etc.).",
      "testStrategy": "Write unit tests for parsing different response formats, including valid JSON, JSON embedded in markdown, and invalid responses. Test validation with Pydantic models for both valid and invalid data structures.",
      "priority": "medium",
      "dependencies": [
        2,
        4
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 7,
      "title": "Implement State Machine Base Class",
      "description": "Create a reusable state machine framework for managing complex processes.",
      "details": "Implement a state machine base class that can be extended for specific workflows:\n\n```python\nfrom typing import Dict, Any, List, Optional, Set, Callable, TypeVar, Generic\nfrom enum import Enum, auto\nimport logging\n\nT = TypeVar('T', bound=Enum)\n\nclass StateMachine(Generic[T]):\n    def __init__(self, initial_state: T):\n        self.current_state = initial_state\n        self.states: Set[T] = {initial_state}\n        self.transitions: Dict[T, Dict[T, Callable[..., bool]]] = {}\n        self.state_handlers: Dict[T, Callable[..., Any]] = {}\n        self.context: Dict[str, Any] = {}\n        self.logger = logging.getLogger(self.__class__.__name__)\n    \n    def add_state(self, state: T, handler: Callable[..., Any]) -> None:\n        self.states.add(state)\n        self.state_handlers[state] = handler\n        if state not in self.transitions:\n            self.transitions[state] = {}\n    \n    def add_transition(self, from_state: T, to_state: T, condition: Callable[..., bool]) -> None:\n        if from_state not in self.states or to_state not in self.states:\n            raise ValueError(\"Both states must be added before defining a transition\")\n        \n        if from_state not in self.transitions:\n            self.transitions[from_state] = {}\n        \n        self.transitions[from_state][to_state] = condition\n    \n    def run(self, **kwargs) -> Any:\n        result = None\n        while True:\n            self.logger.info(f\"Executing state: {self.current_state.name}\")\n            \n            # Execute current state handler\n            handler = self.state_handlers.get(self.current_state)\n            if handler:\n                try:\n                    result = handler(self.context, **kwargs)\n                except Exception as e:\n                    self.logger.error(f\"Error in state {self.current_state.name}: {str(e)}\")\n                    raise\n            \n            # Check for transitions\n            next_state = self._get_next_state(**kwargs)\n            if next_state is None:\n                break  # No valid transition, we're done\n                \n            self.logger.info(f\"Transitioning from {self.current_state.name} to {next_state.name}\")\n            self.current_state = next_state\n        \n        return result\n    \n    def _get_next_state(self, **kwargs) -> Optional[T]:\n        if self.current_state not in self.transitions:\n            return None\n            \n        for next_state, condition in self.transitions[self.current_state].items():\n            if condition(self.context, **kwargs):\n                return next_state\n                \n        return None\n```\n\nThis base class should be flexible enough to handle different types of workflows while providing consistent logging and error handling.",
      "testStrategy": "Create unit tests with a simple test state machine to verify state transitions, handler execution, and error handling. Test both successful workflows and error cases.",
      "priority": "high",
      "dependencies": [
        2
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 8,
      "title": "Implement Analyzer State Machine",
      "description": "Create a state machine implementation for the test analysis workflow.",
      "details": "Implement a concrete state machine for the test analysis workflow:\n\n```python\nfrom enum import Enum, auto\nfrom typing import Dict, Any, List, Optional\n\nclass AnalyzerState(Enum):\n    INITIALIZE = auto()\n    EXTRACT_TEST_RESULTS = auto()\n    ANALYZE_FAILURES = auto()\n    GENERATE_SUGGESTIONS = auto()\n    APPLY_FIXES = auto()\n    COMPLETE = auto()\n    ERROR = auto()\n\nclass AnalyzerStateMachine(StateMachine[AnalyzerState]):\n    def __init__(self, di_container: DIContainer):\n        super().__init__(AnalyzerState.INITIALIZE)\n        self.di_container = di_container\n        \n        # Register states\n        self.add_state(AnalyzerState.INITIALIZE, self._initialize)\n        self.add_state(AnalyzerState.EXTRACT_TEST_RESULTS, self._extract_test_results)\n        self.add_state(AnalyzerState.ANALYZE_FAILURES, self._analyze_failures)\n        self.add_state(AnalyzerState.GENERATE_SUGGESTIONS, self._generate_suggestions)\n        self.add_state(AnalyzerState.APPLY_FIXES, self._apply_fixes)\n        self.add_state(AnalyzerState.COMPLETE, self._complete)\n        self.add_state(AnalyzerState.ERROR, self._handle_error)\n        \n        # Register transitions\n        self.add_transition(\n            AnalyzerState.INITIALIZE, \n            AnalyzerState.EXTRACT_TEST_RESULTS,\n            lambda ctx, **kwargs: ctx.get('initialized', False)\n        )\n        self.add_transition(\n            AnalyzerState.EXTRACT_TEST_RESULTS, \n            AnalyzerState.ANALYZE_FAILURES,\n            lambda ctx, **kwargs: ctx.get('extraction_results') is not None\n        )\n        self.add_transition(\n            AnalyzerState.ANALYZE_FAILURES, \n            AnalyzerState.GENERATE_SUGGESTIONS,\n            lambda ctx, **kwargs: ctx.get('analysis_results') is not None\n        )\n        self.add_transition(\n            AnalyzerState.GENERATE_SUGGESTIONS, \n            AnalyzerState.APPLY_FIXES,\n            lambda ctx, **kwargs: ctx.get('suggestions') is not None and kwargs.get('apply_fixes', False)\n        )\n        self.add_transition(\n            AnalyzerState.GENERATE_SUGGESTIONS, \n            AnalyzerState.COMPLETE,\n            lambda ctx, **kwargs: ctx.get('suggestions') is not None and not kwargs.get('apply_fixes', False)\n        )\n        self.add_transition(\n            AnalyzerState.APPLY_FIXES, \n            AnalyzerState.COMPLETE,\n            lambda ctx, **kwargs: ctx.get('fixes_applied', False)\n        )\n        \n        # Error transitions from any state\n        for state in AnalyzerState:\n            if state not in [AnalyzerState.COMPLETE, AnalyzerState.ERROR]:\n                self.add_transition(\n                    state,\n                    AnalyzerState.ERROR,\n                    lambda ctx, **kwargs: ctx.get('error') is not None\n                )\n    \n    def _initialize(self, context: Dict[str, Any], **kwargs) -> None:\n        try:\n            # Initialize components\n            context['extractor'] = self.di_container.resolve(Extractor)\n            context['analyzer'] = self.di_container.resolve(Analyzer)\n            context['suggester'] = self.di_container.resolve(Suggester)\n            context['applier'] = self.di_container.resolve(Applier)\n            context['initialized'] = True\n        except Exception as e:\n            context['error'] = str(e)\n    \n    def _extract_test_results(self, context: Dict[str, Any], **kwargs) -> None:\n        try:\n            extractor = context['extractor']\n            test_results = kwargs.get('test_results')\n            if not test_results:\n                raise ValueError(\"No test results provided\")\n                \n            context['extraction_results'] = extractor.extract(test_results)\n        except Exception as e:\n            context['error'] = str(e)\n    \n    def _analyze_failures(self, context: Dict[str, Any], **kwargs) -> None:\n        try:\n            analyzer = context['analyzer']\n            extraction_results = context['extraction_results']\n            context['analysis_results'] = analyzer.analyze(extraction_results)\n        except Exception as e:\n            context['error'] = str(e)\n    \n    def _generate_suggestions(self, context: Dict[str, Any], **kwargs) -> None:\n        try:\n            suggester = context['suggester']\n            analysis_results = context['analysis_results']\n            context['suggestions'] = suggester.suggest(analysis_results)\n        except Exception as e:\n            context['error'] = str(e)\n    \n    def _apply_fixes(self, context: Dict[str, Any], **kwargs) -> None:\n        try:\n            applier = context['applier']\n            suggestions = context['suggestions']\n            target_files = kwargs.get('target_files', [])\n            context['fixes_applied'] = applier.apply(suggestions, target_files)\n        except Exception as e:\n            context['error'] = str(e)\n    \n    def _complete(self, context: Dict[str, Any], **kwargs) -> Dict[str, Any]:\n        # Return the final results\n        return {\n            'extraction_results': context.get('extraction_results'),\n            'analysis_results': context.get('analysis_results'),\n            'suggestions': context.get('suggestions'),\n            'fixes_applied': context.get('fixes_applied', False)\n        }\n    \n    def _handle_error(self, context: Dict[str, Any], **kwargs) -> Dict[str, Any]:\n        # Return error information\n        return {\n            'error': context.get('error', 'Unknown error'),\n            'state': self.current_state.name,\n            'partial_results': {\n                'extraction_results': context.get('extraction_results'),\n                'analysis_results': context.get('analysis_results'),\n                'suggestions': context.get('suggestions')\n            }\n        }\n```\n\nThis implementation should handle the complete workflow of extracting test results, analyzing failures, generating suggestions, and optionally applying fixes.",
      "testStrategy": "Create unit tests that verify each state transition and handler. Use mocked dependencies to test both successful and error scenarios. Test the complete workflow with different input parameters.",
      "priority": "high",
      "dependencies": [
        2,
        3,
        4,
        5,
        6,
        7
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 9,
      "title": "Implement Extractor Component",
      "description": "Create the Extractor component for parsing test results according to the Protocol interface.",
      "details": "Implement a concrete Extractor that parses pytest test results:\n\n```python\nfrom typing import Dict, Any, List, Optional\nimport re\nimport os\n\nclass PytestExtractor:\n    def extract(self, test_results: str) -> Dict[str, Any]:\n        \"\"\"Extract relevant information from pytest test results.\"\"\"\n        failures = []\n        test_files = set()\n        \n        # Extract test failures using regex patterns\n        failure_pattern = r'(FAILED|ERROR) (.+?)::(.+?)\\s'\n        for match in re.finditer(failure_pattern, test_results):\n            status, file_path, test_name = match.groups()\n            \n            # Find the error message and traceback\n            error_section = self._extract_error_section(test_results, file_path, test_name)\n            \n            failures.append({\n                'status': status,\n                'file_path': file_path,\n                'test_name': test_name,\n                'error': error_section.get('error', ''),\n                'traceback': error_section.get('traceback', ''),\n                'code_context': self._extract_code_context(file_path, error_section)\n            })\n            \n            test_files.add(file_path)\n        \n        return {\n            'failures': failures,\n            'test_files': list(test_files),\n            'raw_output': test_results\n        }\n    \n    def _extract_error_section(self, test_results: str, file_path: str, test_name: str) -> Dict[str, str]:\n        # Find the section containing the error for this specific test\n        pattern = f\"{file_path}::{test_name}[\\s\\S]+?(?=\\n\\n|$)\"\n        match = re.search(pattern, test_results)\n        if not match:\n            return {'error': '', 'traceback': ''}\n            \n        section = match.group(0)\n        \n        # Extract traceback and error message\n        traceback_lines = []\n        error_message = ''\n        \n        in_traceback = False\n        for line in section.split('\\n'):\n            if line.strip().startswith('E '):\n                if 'Error:' in line or 'Exception:' in line or 'AssertionError:' in line:\n                    error_message = line.strip()[2:]\n                else:\n                    traceback_lines.append(line.strip()[2:])\n            elif line.strip().startswith('_ _ _'):\n                in_traceback = True\n            elif in_traceback and line.strip():\n                traceback_lines.append(line.strip())\n        \n        return {\n            'error': error_message,\n            'traceback': '\\n'.join(traceback_lines)\n        }\n    \n    def _extract_code_context(self, file_path: str, error_section: Dict[str, str]) -> Dict[str, Any]:\n        # Extract line numbers from traceback\n        line_pattern = f\"{file_path}:(\\d+)\"\n        line_matches = re.findall(line_pattern, error_section.get('traceback', ''))\n        \n        if not line_matches or not os.path.exists(file_path):\n            return {'source_code': '', 'line_number': 0}\n        \n        try:\n            line_number = int(line_matches[0])\n            with open(file_path, 'r') as f:\n                lines = f.readlines()\n            \n            # Get context (5 lines before and after)\n            start = max(0, line_number - 6)\n            end = min(len(lines), line_number + 5)\n            context_lines = lines[start:end]\n            \n            return {\n                'source_code': ''.join(context_lines),\n                'line_number': line_number,\n                'start_line': start + 1  # 1-based line numbering\n            }\n        except Exception as e:\n            return {'source_code': f\"Error extracting code: {str(e)}\", 'line_number': 0}\n```\n\nThis implementation should handle parsing pytest output to extract test failures, error messages, tracebacks, and code context.",
      "testStrategy": "Create unit tests with sample pytest output to verify extraction of failures, error messages, and code context. Test with different pytest output formats and error types. Verify handling of edge cases like missing files or malformed output.",
      "priority": "medium",
      "dependencies": [
        2,
        3
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 10,
      "title": "Implement Analyzer Component",
      "description": "Create the Analyzer component for analyzing test failures according to the Protocol interface.",
      "details": "Implement a concrete Analyzer that analyzes test failures using the LLM service:\n\n```python\nfrom typing import Dict, Any, List, Optional\n\nclass TestFailureAnalyzer:\n    def __init__(self, llm_service: LLMService, prompt_builder: PromptBuilder, response_parser: ResponseParser):\n        self.llm_service = llm_service\n        self.prompt_builder = prompt_builder\n        self.response_parser = response_parser\n    \n    def analyze(self, extraction_results: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Analyze test failures and determine root causes.\"\"\"\n        failures = extraction_results.get('failures', [])\n        if not failures:\n            return {'analyses': [], 'summary': 'No failures to analyze'}\n        \n        analyses = []\n        for failure in failures:\n            analysis = self._analyze_failure(failure)\n            analyses.append({\n                'failure': failure,\n                'analysis': analysis\n            })\n        \n        # Generate overall summary\n        summary = self._generate_summary(analyses)\n        \n        return {\n            'analyses': analyses,\n            'summary': summary,\n            'extraction_results': extraction_results\n        }\n    \n    def _analyze_failure(self, failure: Dict[str, Any]) -> Dict[str, Any]:\n        # Build prompt for the LLM\n        prompt = self.prompt_builder.build_prompt(\n            'analyze_failure',\n            file_path=failure.get('file_path', ''),\n            test_name=failure.get('test_name', ''),\n            error=failure.get('error', ''),\n            traceback=failure.get('traceback', ''),\n            code_context=failure.get('code_context', {}).get('source_code', '')\n        )\n        \n        # Get analysis from LLM\n        response = self.llm_service.generate(prompt)\n        \n        try:\n            # Parse the response\n            analysis = self.response_parser.parse_json(response)\n            return analysis.dict()\n        except Exception as e:\n            # Fallback to raw response if parsing fails\n            return {\n                'root_cause': 'Failed to parse analysis',\n                'explanation': str(e),\n                'raw_response': response\n            }\n    \n    def _generate_summary(self, analyses: List[Dict[str, Any]]) -> str:\n        # Count common root causes\n        root_causes = {}\n        for item in analyses:\n            cause = item.get('analysis', {}).get('root_cause', 'Unknown')\n            root_causes[cause] = root_causes.get(cause, 0) + 1\n        \n        # Generate summary text\n        summary_lines = [f\"Analyzed {len(analyses)} test failures:\"]\n        for cause, count in sorted(root_causes.items(), key=lambda x: x[1], reverse=True):\n            summary_lines.append(f\"- {cause}: {count} occurrences\")\n        \n        return '\\n'.join(summary_lines)\n```\n\nThis implementation should handle analyzing test failures by sending them to the LLM service and parsing the responses.",
      "testStrategy": "Create unit tests with mocked LLM service to verify analysis of different types of test failures. Test both successful analyses and error handling. Verify the summary generation with different sets of analyses.",
      "priority": "medium",
      "dependencies": [
        2,
        3,
        4,
        5,
        6
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 11,
      "title": "Implement Suggester Component",
      "description": "Create the Suggester component for suggesting fixes according to the Protocol interface.",
      "details": "Implement a concrete Suggester that suggests fixes for test failures using the LLM service:\n\n```python\nfrom typing import Dict, Any, List, Optional\n\nclass FixSuggester:\n    def __init__(self, llm_service: LLMService, prompt_builder: PromptBuilder, response_parser: ResponseParser):\n        self.llm_service = llm_service\n        self.prompt_builder = prompt_builder\n        self.response_parser = response_parser\n    \n    def suggest(self, analysis_results: Dict[str, Any]) -> List[Dict[str, Any]]:\n        \"\"\"Suggest fixes for analyzed test failures.\"\"\"\n        analyses = analysis_results.get('analyses', [])\n        if not analyses:\n            return []\n        \n        suggestions = []\n        for analysis_item in analyses:\n            failure = analysis_item.get('failure', {})\n            analysis = analysis_item.get('analysis', {})\n            \n            suggestion = self._generate_suggestion(failure, analysis)\n            suggestions.append(suggestion)\n        \n        return suggestions\n    \n    def _generate_suggestion(self, failure: Dict[str, Any], analysis: Dict[str, Any]) -> Dict[str, Any]:\n        # Build prompt for the LLM\n        prompt = self.prompt_builder.build_prompt(\n            'suggest_fix',\n            file_path=failure.get('file_path', ''),\n            test_name=failure.get('test_name', ''),\n            error=failure.get('error', ''),\n            traceback=failure.get('traceback', ''),\n            code_context=failure.get('code_context', {}).get('source_code', ''),\n            root_cause=analysis.get('root_cause', ''),\n            explanation=analysis.get('explanation', '')\n        )\n        \n        # Get suggestion from LLM\n        response = self.llm_service.generate(prompt)\n        \n        try:\n            # Parse the response\n            suggestion_model = self.response_parser.parse_json(response)\n            suggestion_dict = suggestion_model.dict()\n            \n            # Add metadata\n            return {\n                **suggestion_dict,\n                'file_path': failure.get('file_path', ''),\n                'test_name': failure.get('test_name', ''),\n                'confidence': self._calculate_confidence(suggestion_dict)\n            }\n        except Exception as e:\n            # Fallback to raw response if parsing fails\n            return {\n                'file_path': failure.get('file_path', ''),\n                'test_name': failure.get('test_name', ''),\n                'fix_type': 'unknown',\n                'description': 'Failed to parse suggestion',\n                'code_changes': [],\n                'confidence': 0.0,\n                'error': str(e),\n                'raw_response': response\n            }\n    \n    def _calculate_confidence(self, suggestion: Dict[str, Any]) -> float:\n        # Simple confidence calculation based on suggestion properties\n        confidence = 0.5  # Base confidence\n        \n        # Adjust based on fix type\n        fix_type = suggestion.get('fix_type', '').lower()\n        if fix_type in ['simple', 'straightforward', 'obvious']:\n            confidence += 0.3\n        elif fix_type in ['complex', 'uncertain']:\n            confidence -= 0.2\n        \n        # Adjust based on code changes\n        code_changes = suggestion.get('code_changes', [])\n        if len(code_changes) == 0:\n            confidence -= 0.3\n        elif len(code_changes) > 5:\n            confidence -= 0.1  # More changes = more uncertainty\n        \n        # Ensure confidence is between 0 and 1\n        return max(0.0, min(1.0, confidence))\n```\n\nThis implementation should handle generating fix suggestions for test failures by sending analysis results to the LLM service and parsing the responses.",
      "testStrategy": "Create unit tests with mocked LLM service to verify suggestion generation for different types of test failures and analyses. Test both successful suggestions and error handling. Verify confidence calculation with different suggestion properties.",
      "priority": "medium",
      "dependencies": [
        2,
        3,
        4,
        5,
        6
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 12,
      "title": "Implement Applier Component",
      "description": "Create the Applier component for applying suggested fixes according to the Protocol interface.",
      "details": "Implement a concrete Applier that applies suggested fixes to test files:\n\n```python\nfrom typing import Dict, Any, List, Optional\nimport os\nimport re\nfrom difflib import unified_diff\n\nclass FixApplier:\n    def apply(self, suggestions: List[Dict[str, Any]], target_files: List[str] = None) -> bool:\n        \"\"\"Apply suggested fixes to target files.\"\"\"\n        if not suggestions:\n            return False\n        \n        # Filter suggestions by target files if specified\n        if target_files:\n            suggestions = [s for s in suggestions if s.get('file_path') in target_files]\n        \n        if not suggestions:\n            return False\n        \n        success_count = 0\n        for suggestion in suggestions:\n            if self._apply_suggestion(suggestion):\n                success_count += 1\n        \n        return success_count > 0\n    \n    def _apply_suggestion(self, suggestion: Dict[str, Any]) -> bool:\n        file_path = suggestion.get('file_path')\n        if not file_path or not os.path.exists(file_path):\n            return False\n        \n        code_changes = suggestion.get('code_changes', [])\n        if not code_changes:\n            return False\n        \n        try:\n            # Read the original file\n            with open(file_path, 'r') as f:\n                original_lines = f.readlines()\n            \n            # Apply changes\n            new_lines = original_lines.copy()\n            for change in code_changes:\n                change_type = change.get('type', '').lower()\n                if change_type == 'replace':\n                    self._apply_replace(new_lines, change)\n                elif change_type == 'insert':\n                    self._apply_insert(new_lines, change)\n                elif change_type == 'delete':\n                    self._apply_delete(new_lines, change)\n            \n            # Only write if there were actual changes\n            if new_lines != original_lines:\n                # Create backup\n                backup_path = f\"{file_path}.bak\"\n                with open(backup_path, 'w') as f:\n                    f.writelines(original_lines)\n                \n                # Write new content\n                with open(file_path, 'w') as f:\n                    f.writelines(new_lines)\n                \n                return True\n            \n            return False\n        except Exception as e:\n            # Log the error and return False\n            print(f\"Error applying fix to {file_path}: {str(e)}\")\n            return False\n    \n    def _apply_replace(self, lines: List[str], change: Dict[str, Any]) -> None:\n        start_line = change.get('start_line', 0) - 1  # Convert to 0-based index\n        end_line = change.get('end_line', 0) - 1\n        new_code = change.get('new_code', '')\n        \n        if start_line < 0 or end_line >= len(lines) or start_line > end_line:\n            return\n        \n        # Replace the specified lines with new code\n        new_lines = new_code.split('\\n')\n        if new_lines[-1] == '':\n            new_lines = new_lines[:-1]  # Remove trailing empty line\n            \n        # Ensure each line ends with a newline\n        new_lines = [line if line.endswith('\\n') else line + '\\n' for line in new_lines]\n        \n        lines[start_line:end_line+1] = new_lines\n    \n    def _apply_insert(self, lines: List[str], change: Dict[str, Any]) -> None:\n        line_number = change.get('line_number', 0) - 1  # Convert to 0-based index\n        new_code = change.get('new_code', '')\n        \n        if line_number < 0 or line_number > len(lines):\n            return\n        \n        # Insert new code at the specified line\n        new_lines = new_code.split('\\n')\n        if new_lines[-1] == '':\n            new_lines = new_lines[:-1]  # Remove trailing empty line\n            \n        # Ensure each line ends with a newline\n        new_lines = [line if line.endswith('\\n') else line + '\\n' for line in new_lines]\n        \n        for i, line in enumerate(new_lines):\n            lines.insert(line_number + i, line)\n    \n    def _apply_delete(self, lines: List[str], change: Dict[str, Any]) -> None:\n        start_line = change.get('start_line', 0) - 1  # Convert to 0-based index\n        end_line = change.get('end_line', 0) - 1\n        \n        if start_line < 0 or end_line >= len(lines) or start_line > end_line:\n            return\n        \n        # Delete the specified lines\n        del lines[start_line:end_line+1]\n```\n\nThis implementation should handle applying suggested fixes to test files, including replacing, inserting, and deleting code.",
      "testStrategy": "Create unit tests with temporary test files to verify applying different types of changes (replace, insert, delete). Test both successful applications and error handling. Verify file backups are created correctly.",
      "priority": "medium",
      "dependencies": [
        2,
        3
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 13,
      "title": "Implement Facade for Backward Compatibility",
      "description": "Create a facade that maintains backward compatibility with the existing API while using the new architecture internally.",
      "details": "Implement a facade that provides the same interface as the original code but uses the new components internally:\n\n```python\nfrom typing import Dict, Any, List, Optional, Union\nimport os\nimport json\n\nclass PytestAnalyzerFacade:\n    def __init__(self, di_container: DIContainer = None):\n        if di_container is None:\n            # Create and configure a default container\n            di_container = self._create_default_container()\n        \n        self.di_container = di_container\n    \n    def _create_default_container(self) -> DIContainer:\n        container = DIContainer()\n        \n        # Create default configuration\n        llm_config = LLMConfig(\n            api_key=os.environ.get('OPENAI_API_KEY', ''),\n            model_name='gpt-3.5-turbo',\n            temperature=0.7,\n            max_tokens=1000\n        )\n        \n        # Register components\n        container.register(LLMService, LLMService(llm_config))\n        container.register(PromptBuilder, PromptBuilder())\n        container.register(Extractor, PytestExtractor())\n        \n        # Register factories for components that need dependencies\n        container.register_factory(Analyzer, lambda: TestFailureAnalyzer(\n            container.resolve(LLMService),\n            container.resolve(PromptBuilder),\n            ResponseParser(AnalysisModel)\n        ))\n        \n        container.register_factory(Suggester, lambda: FixSuggester(\n            container.resolve(LLMService),\n            container.resolve(PromptBuilder),\n            ResponseParser(SuggestionModel)\n        ))\n        \n        container.register(Applier, FixApplier())\n        \n        return container\n    \n    def analyze_test_results(self, test_output: str) -> Dict[str, Any]:\n        \"\"\"Analyze pytest test results and return analysis.\"\"\"\n        # Create and run the state machine\n        state_machine = AnalyzerStateMachine(self.di_container)\n        result = state_machine.run(test_results=test_output, apply_fixes=False)\n        \n        if 'error' in result:\n            # Handle error case\n            return {\n                'success': False,\n                'error': result['error'],\n                'analyses': [],\n                'suggestions': []\n            }\n        \n        return {\n            'success': True,\n            'analyses': result.get('analysis_results', {}).get('analyses', []),\n            'suggestions': result.get('suggestions', [])\n        }\n    \n    def suggest_fixes(self, test_output: str) -> List[Dict[str, Any]]:\n        \"\"\"Analyze test results and suggest fixes.\"\"\"\n        result = self.analyze_test_results(test_output)\n        return result.get('suggestions', [])\n    \n    def apply_fixes(self, test_output: str, target_files: List[str] = None) -> Dict[str, Any]:\n        \"\"\"Analyze test results, suggest and apply fixes.\"\"\"\n        # Create and run the state machine\n        state_machine = AnalyzerStateMachine(self.di_container)\n        result = state_machine.run(\n            test_results=test_output,\n            apply_fixes=True,\n            target_files=target_files\n        )\n        \n        if 'error' in result:\n            # Handle error case\n            return {\n                'success': False,\n                'error': result['error'],\n                'fixes_applied': False\n            }\n        \n        return {\n            'success': True,\n            'fixes_applied': result.get('fixes_applied', False),\n            'suggestions': result.get('suggestions', [])\n        }\n```\n\nThis facade should provide the same interface as the original code while using the new architecture internally, ensuring backward compatibility.",
      "testStrategy": "Create unit tests that verify the facade provides the same interface as the original code. Test with different inputs and verify the outputs match the expected format. Test both successful and error scenarios.",
      "priority": "high",
      "dependencies": [
        3,
        8,
        9,
        10,
        11,
        12
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 14,
      "title": "Implement Comprehensive Error Handling",
      "description": "Implement a consistent error handling mechanism throughout the codebase.",
      "details": "Implement a consistent error handling mechanism:\n\n```python\nfrom typing import Dict, Any, Optional, Type, TypeVar, Generic\nimport logging\nimport traceback\nfrom contextlib import contextmanager\n\nT = TypeVar('T')\n\nclass PytestAnalyzerError(Exception):\n    \"\"\"Base exception class for pytest-analyzer errors.\"\"\"\n    def __init__(self, message: str, cause: Optional[Exception] = None):\n        self.message = message\n        self.cause = cause\n        super().__init__(message)\n\nclass ConfigurationError(PytestAnalyzerError):\n    \"\"\"Error raised when there's a configuration issue.\"\"\"\n    pass\n\nclass ExtractionError(PytestAnalyzerError):\n    \"\"\"Error raised during test result extraction.\"\"\"\n    pass\n\nclass AnalysisError(PytestAnalyzerError):\n    \"\"\"Error raised during test failure analysis.\"\"\"\n    pass\n\nclass SuggestionError(PytestAnalyzerError):\n    \"\"\"Error raised during fix suggestion generation.\"\"\"\n    pass\n\nclass ApplicationError(PytestAnalyzerError):\n    \"\"\"Error raised during fix application.\"\"\"\n    pass\n\nclass LLMServiceError(PytestAnalyzerError):\n    \"\"\"Error raised during LLM service operations.\"\"\"\n    pass\n\nclass ErrorHandler:\n    def __init__(self, logger: Optional[logging.Logger] = None):\n        self.logger = logger or logging.getLogger('pytest-analyzer')\n    \n    @contextmanager\n    def handle_errors(self, error_type: Type[PytestAnalyzerError], operation: str, reraise: bool = True):\n        \"\"\"Context manager for handling errors in a consistent way.\"\"\"\n        try:\n            yield\n        except PytestAnalyzerError as e:\n            # Already a known error type, just log and re-raise\n            self.logger.error(f\"{operation} failed: {e.message}\")\n            if e.cause:\n                self.logger.debug(f\"Caused by: {str(e.cause)}\\n{traceback.format_exc()}\")\n            if reraise:\n                raise\n        except Exception as e:\n            # Wrap unknown exceptions in the specified error type\n            error_message = f\"{operation} failed: {str(e)}\"\n            self.logger.error(error_message)\n            self.logger.debug(f\"Exception details: {traceback.format_exc()}\")\n            if reraise:\n                raise error_type(error_message, e)\n\nclass ResultWithError(Generic[T]):\n    \"\"\"A container for operation results that might include errors.\"\"\"\n    def __init__(self, value: Optional[T] = None, error: Optional[PytestAnalyzerError] = None):\n        self.value = value\n        self.error = error\n    \n    @property\n    def success(self) -> bool:\n        return self.error is None\n    \n    def unwrap(self) -> T:\n        \"\"\"Get the value or raise the error if present.\"\"\"\n        if self.error:\n            raise self.error\n        return self.value\n    \n    def map(self, func):\n        \"\"\"Apply a function to the value if no error, otherwise return self.\"\"\"\n        if self.error:\n            return self\n        try:\n            return ResultWithError(func(self.value))\n        except Exception as e:\n            if isinstance(e, PytestAnalyzerError):\n                return ResultWithError(error=e)\n            return ResultWithError(error=PytestAnalyzerError(str(e), e))\n```\n\nUpdate all components to use this error handling mechanism. For example, update the LLM service:\n\n```python\nclass LLMService:\n    def __init__(self, config: LLMConfig, error_handler: Optional[ErrorHandler] = None):\n        self.config = config\n        self.error_handler = error_handler or ErrorHandler()\n    \n    def generate(self, prompt: str, **kwargs) -> ResultWithError[str]:\n        with self.error_handler.handle_errors(LLMServiceError, \"LLM generation\", reraise=False) as result:\n            # Implementation for synchronous API call\n            params = {**self.config.__dict__, **kwargs}\n            response = requests.post(\n                \"https://api.openai.com/v1/completions\",\n                headers={\"Authorization\": f\"Bearer {self.config.api_key}\"},\n                json={\"prompt\": prompt, **params}\n            )\n            response.raise_for_status()\n            return ResultWithError(response.json()[\"choices\"][0][\"text\"])\n        \n        # If we get here, there was an error\n        return result\n    \n    async def generate_async(self, prompt: str, **kwargs) -> ResultWithError[str]:\n        with self.error_handler.handle_errors(LLMServiceError, \"Async LLM generation\", reraise=False) as result:\n            # Implementation for asynchronous API call\n            params = {**self.config.__dict__, **kwargs}\n            async with aiohttp.ClientSession() as session:\n                async with session.post(\n                    \"https://api.openai.com/v1/completions\",\n                    headers={\"Authorization\": f\"Bearer {self.config.api_key}\"},\n                    json={\"prompt\": prompt, **params}\n                ) as response:\n                    if response.status != 200:\n                        text = await response.text()\n                        raise LLMServiceError(f\"API error: {response.status}, {text}\")\n                    data = await response.json()\n                    return ResultWithError(data[\"choices\"][0][\"text\"])\n        \n        # If we get here, there was an error\n        return result\n```\n\nUpdate other components similarly to use the error handling mechanism.",
      "testStrategy": "Create unit tests for the error handling mechanism, including the context manager and ResultWithError class. Test error propagation through component chains. Verify that errors are properly logged and wrapped in the appropriate error types.",
      "priority": "medium",
      "dependencies": [
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9,
        10,
        11,
        12,
        13
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 15,
      "title": "Implement Comprehensive Testing Suite",
      "description": "Create a comprehensive testing suite for all components and workflows.",
      "details": "Implement a comprehensive testing suite that covers all components and workflows:\n\n1. **Unit Tests**:\n   - Create test files for each component\n   - Use pytest fixtures for common test data and mocks\n   - Test both success and error paths\n   - Use parameterized tests for different scenarios\n\n2. **Integration Tests**:\n   - Test component chains (e.g., Extractor -> Analyzer -> Suggester)\n   - Test the state machine with different inputs\n   - Test the facade with different scenarios\n\n3. **End-to-End Tests**:\n   - Test the complete workflow with real-world examples\n   - Test backward compatibility with existing code\n\n4. **Test Helpers**:\n   - Create mock implementations of all interfaces for testing\n   - Create test fixtures for common test data\n   - Create test utilities for common assertions\n\nExample test file structure:\n\n```\ntests/\n  unit/\n    test_extractor.py\n    test_analyzer.py\n    test_suggester.py\n    test_applier.py\n    test_llm_service.py\n    test_prompt_builder.py\n    test_response_parser.py\n    test_state_machine.py\n    test_di_container.py\n    test_error_handling.py\n  integration/\n    test_analysis_workflow.py\n    test_suggestion_workflow.py\n    test_application_workflow.py\n  e2e/\n    test_facade.py\n    test_backward_compatibility.py\n  conftest.py  # Common fixtures\n  test_data/   # Test data files\n```\n\nExample unit test for the Extractor:\n\n```python\nimport pytest\nfrom pytest_analyzer.extractors import PytestExtractor\n\n@pytest.fixture\ndef sample_pytest_output():\n    with open('tests/test_data/sample_pytest_output.txt', 'r') as f:\n        return f.read()\n\ndef test_extractor_with_valid_output(sample_pytest_output):\n    # Arrange\n    extractor = PytestExtractor()\n    \n    # Act\n    result = extractor.extract(sample_pytest_output)\n    \n    # Assert\n    assert 'failures' in result\n    assert isinstance(result['failures'], list)\n    assert len(result['failures']) > 0\n    \n    # Check first failure structure\n    failure = result['failures'][0]\n    assert 'file_path' in failure\n    assert 'test_name' in failure\n    assert 'error' in failure\n    assert 'traceback' in failure\n    assert 'code_context' in failure\n\ndef test_extractor_with_empty_output():\n    # Arrange\n    extractor = PytestExtractor()\n    \n    # Act\n    result = extractor.extract('')\n    \n    # Assert\n    assert 'failures' in result\n    assert len(result['failures']) == 0\n    assert 'test_files' in result\n    assert len(result['test_files']) == 0\n\ndef test_extractor_error_handling():\n    # Arrange\n    extractor = PytestExtractor()\n    \n    # Act & Assert\n    with pytest.raises(ExtractionError):\n        extractor.extract(None)  # Should raise for None input\n```\n\nExample integration test for the analysis workflow:\n\n```python\nimport pytest\nfrom pytest_analyzer.di import DIContainer\nfrom pytest_analyzer.extractors import PytestExtractor\nfrom pytest_analyzer.analyzers import TestFailureAnalyzer\nfrom pytest_analyzer.llm import LLMService, LLMConfig\nfrom pytest_analyzer.prompts import PromptBuilder\nfrom pytest_analyzer.parsers import ResponseParser, AnalysisModel\n\n@pytest.fixture\ndef mock_llm_service(monkeypatch):\n    class MockLLMService:\n        def generate(self, prompt, **kwargs):\n            # Return a predefined response based on the prompt\n            if 'analyze_failure' in prompt:\n                return '{\"root_cause\": \"Test error\", \"explanation\": \"This is a test\"}'\n            return '{}'\n    \n    return MockLLMService()\n\n@pytest.fixture\ndef di_container(mock_llm_service):\n    container = DIContainer()\n    container.register(LLMService, mock_llm_service)\n    container.register(PromptBuilder, PromptBuilder())\n    container.register(Extractor, PytestExtractor())\n    container.register_factory(Analyzer, lambda: TestFailureAnalyzer(\n        container.resolve(LLMService),\n        container.resolve(PromptBuilder),\n        ResponseParser(AnalysisModel)\n    ))\n    return container\n\ndef test_analysis_workflow(di_container, sample_pytest_output):\n    # Arrange\n    extractor = di_container.resolve(Extractor)\n    analyzer = di_container.resolve(Analyzer)\n    \n    # Act\n    extraction_results = extractor.extract(sample_pytest_output)\n    analysis_results = analyzer.analyze(extraction_results)\n    \n    # Assert\n    assert 'analyses' in analysis_results\n    assert isinstance(analysis_results['analyses'], list)\n    assert len(analysis_results['analyses']) > 0\n    \n    # Check first analysis structure\n    analysis = analysis_results['analyses'][0]\n    assert 'failure' in analysis\n    assert 'analysis' in analysis\n    assert 'root_cause' in analysis['analysis']\n    assert 'explanation' in analysis['analysis']\n```\n\nImplement similar tests for all components and workflows.",
      "testStrategy": "Run the tests with pytest and verify that all tests pass. Use pytest-cov to measure test coverage and ensure it meets the 80% target. Verify that all components and workflows are properly tested, including error cases.",
      "priority": "high",
      "dependencies": [
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9,
        10,
        11,
        12,
        13,
        14
      ],
      "status": "pending",
      "subtasks": []
    }
  ]
}
