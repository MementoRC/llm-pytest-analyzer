{
  "tasks": [
    {
      "id": 1,
      "title": "Create New Directory Structure for DDD Architecture",
      "description": "Implement the new Domain-Driven Design directory structure as specified in the PRD, creating the domain, infrastructure, application, interfaces, and cross-cutting layers.",
      "details": "Create the following directory structure:\n\n```\nsrc/pytest_analyzer/core/\n├── domain/\n│   ├── entities/           # Core business objects (PytestFailure, FixSuggestion)\n│   ├── services/           # Domain services (FailureAnalyzer, FixSuggester)\n│   ├── repositories/       # Data access abstractions\n│   └── value_objects/      # Immutable domain concepts\n├── infrastructure/\n│   ├── llm/               # External LLM service integrations\n│   ├── environment/       # Environment manager implementations\n│   ├── extraction/        # Report parsing implementations\n│   ├── di/               # Dependency injection container\n│   └── persistence/       # File system, caching implementations\n├── application/\n│   ├── services/          # Application services (AnalyzerService)\n│   ├── workflows/         # State machines and orchestration\n│   └── facades/           # Public API facades\n├── interfaces/\n│   ├── protocols.py       # All protocol definitions\n│   ├── errors.py          # Exception definitions\n│   └── types.py           # Type definitions\n└── cross_cutting/\n    ├── logging/            # Centralized logging configuration\n    ├── monitoring/         # Health checks and metrics\n    ├── caching/           # Result caching strategies\n    ├── validation/        # Input/output validation\n    └── configuration/     # Configuration management\n```\n\nEnsure all directories have proper `__init__.py` files to maintain package structure. Create placeholder README.md files in each directory explaining its purpose according to DDD principles.",
      "testStrategy": "Verify directory structure exists with correct hierarchy. Ensure all __init__.py files are properly created. Validate import paths work correctly by attempting to import from each new module.",
      "priority": "high",
      "dependencies": [],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 2,
      "title": "Implement BaseEnvironmentManager Abstract Class",
      "description": "Create a base abstract class for environment managers to eliminate code duplication across Poetry, Pixi, Hatch, UV, and Pipenv managers.",
      "details": "Create `src/pytest_analyzer/core/infrastructure/environment/base_manager.py` with the following implementation:\n\n```python\nfrom abc import ABC, abstractmethod\nfrom pathlib import Path\nimport subprocess\nfrom typing import List, Optional\nimport logging\n\nclass BaseEnvironmentManager(ABC):\n    \"\"\"Base class for all environment managers to eliminate code duplication.\"\"\"\n    \n    def __init__(self, project_path: Path):\n        self.project_path = project_path\n        self.logger = logging.getLogger(self.__class__.__name__)\n    \n    def execute_command(self, command: List[str]) -> int:\n        \"\"\"Common implementation for executing commands in the project directory.\"\"\"\n        self.logger.debug(f\"Executing command: {' '.join(command)}\")\n        return subprocess.call(command, cwd=self.project_path)\n    \n    def activate(self) -> None:\n        \"\"\"Default no-op implementation for environment activation.\"\"\"\n        self.logger.debug(\"Environment activation not required\")\n        pass\n    \n    def deactivate(self) -> None:\n        \"\"\"Default no-op implementation for environment deactivation.\"\"\"\n        self.logger.debug(\"Environment deactivation not required\")\n        pass\n    \n    @abstractmethod\n    def build_command(self, base_command: List[str]) -> List[str]:\n        \"\"\"Manager-specific command building logic to be implemented by subclasses.\"\"\"\n        pass\n```\n\nThis base class extracts the common functionality identified in the PRD from the existing environment manager implementations.",
      "testStrategy": "Create unit tests for BaseEnvironmentManager that verify:\n1. Constructor properly initializes project_path\n2. execute_command correctly calls subprocess.call with the right arguments\n3. Default activate/deactivate methods don't raise exceptions\n4. Attempting to instantiate the abstract class directly raises TypeError\n5. Subclassing without implementing build_command raises TypeError",
      "priority": "high",
      "dependencies": [
        1
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 3,
      "title": "Implement BaseFactory Abstract Class",
      "description": "Create a base factory class to consolidate repeated factory initialization patterns across extractor, LLM, and suggester factories.",
      "details": "Create `src/pytest_analyzer/core/infrastructure/base_factory.py` with the following implementation:\n\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import Optional, Dict, Any, Type, TypeVar\nimport logging\nfrom pathlib import Path\n\n# Import the Settings class from the appropriate location\nfrom pytest_analyzer.core.cross_cutting.configuration.settings import Settings\n\nT = TypeVar('T')\n\nclass BaseFactory(ABC):\n    \"\"\"Base factory class to eliminate duplication across factory implementations.\"\"\"\n    \n    def __init__(self, settings: Optional[Settings] = None):\n        self.settings = settings or Settings()\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self._registry: Dict[str, Type[T]] = {}\n    \n    def register(self, key: str, implementation: Type[T]) -> None:\n        \"\"\"Register an implementation with a specific key.\"\"\"\n        self.logger.debug(f\"Registering {implementation.__name__} with key '{key}'\")\n        self._registry[key] = implementation\n    \n    def get_implementation(self, key: str) -> Type[T]:\n        \"\"\"Get the implementation for a specific key.\"\"\"\n        if key not in self._registry:\n            self.logger.error(f\"No implementation registered for key '{key}'\")\n            raise KeyError(f\"No implementation registered for key '{key}'\")\n        return self._registry[key]\n    \n    def _detect_file_type(self, file_path: str) -> str:\n        \"\"\"Common file detection logic based on file extension or content.\"\"\"\n        path = Path(file_path)\n        return path.suffix.lower()[1:] if path.suffix else \"\"\n    \n    @abstractmethod\n    def create(self, *args, **kwargs) -> T:\n        \"\"\"Create an instance of the appropriate implementation.\"\"\"\n        pass\n```\n\nThis base factory consolidates common patterns identified in the PRD from existing factory implementations.",
      "testStrategy": "Create unit tests for BaseFactory that verify:\n1. Constructor properly initializes settings and logger\n2. register method adds implementations to the registry\n3. get_implementation returns the correct implementation or raises KeyError\n4. _detect_file_type correctly extracts file extensions\n5. Attempting to instantiate the abstract class directly raises TypeError\n6. Subclassing without implementing create raises TypeError",
      "priority": "high",
      "dependencies": [
        1
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 4,
      "title": "Create BaseLLMService for Common LLM Service Logic",
      "description": "Extract common initialization code from sync and async LLM services into a shared base class to eliminate duplication.",
      "details": "Create `src/pytest_analyzer/core/infrastructure/llm/base_llm_service.py` with the following implementation:\n\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Any, Optional, List\nimport logging\n\n# Import from the new structure\nfrom pytest_analyzer.core.cross_cutting.configuration.settings import Settings\nfrom pytest_analyzer.core.interfaces.protocols import LLMProvider\n\nclass BaseLLMService(ABC):\n    \"\"\"Base class for LLM services to eliminate code duplication between sync and async implementations.\"\"\"\n    \n    def __init__(self, provider: Optional[LLMProvider] = None, settings: Optional[Settings] = None):\n        self.settings = settings or Settings()\n        self.provider = provider or self._create_default_provider()\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self.model = self.settings.get(\"llm.model\", \"gpt-3.5-turbo\")\n        self.temperature = float(self.settings.get(\"llm.temperature\", \"0.7\"))\n        self.max_tokens = int(self.settings.get(\"llm.max_tokens\", \"2000\"))\n        self.timeout = int(self.settings.get(\"llm.timeout_seconds\", \"30\"))\n    \n    def _create_default_provider(self) -> LLMProvider:\n        \"\"\"Create the default LLM provider based on settings.\"\"\"\n        provider_name = self.settings.get(\"llm.provider\", \"openai\")\n        self.logger.info(f\"Creating default LLM provider: {provider_name}\")\n        \n        # This would be implemented in subclasses or use a factory pattern\n        raise NotImplementedError(\"Subclasses must implement _create_default_provider\")\n    \n    def _prepare_messages(self, prompt: str, context: Optional[Dict[str, Any]] = None) -> List[Dict[str, str]]:\n        \"\"\"Prepare messages for the LLM provider with consistent formatting.\"\"\"\n        messages = [\n            {\"role\": \"system\", \"content\": self._get_system_prompt(context)},\n            {\"role\": \"user\", \"content\": prompt}\n        ]\n        return messages\n    \n    def _get_system_prompt(self, context: Optional[Dict[str, Any]] = None) -> str:\n        \"\"\"Get the system prompt, potentially customized based on context.\"\"\"\n        return self.settings.get(\"llm.system_prompt\", \"You are a helpful assistant.\")\n    \n    @abstractmethod\n    def generate(self, prompt: str, context: Optional[Dict[str, Any]] = None) -> str:\n        \"\"\"Generate a response from the LLM based on the prompt and context.\"\"\"\n        pass\n```\n\nThis base class extracts the common initialization and configuration logic from the existing LLM service implementations.",
      "testStrategy": "Create unit tests for BaseLLMService that verify:\n1. Constructor properly initializes all properties from settings\n2. _prepare_messages correctly formats the messages array\n3. _get_system_prompt returns the expected system prompt\n4. Attempting to instantiate the abstract class directly raises TypeError\n5. Subclassing without implementing generate and _create_default_provider raises TypeError",
      "priority": "medium",
      "dependencies": [
        1
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 5,
      "title": "Implement Error Handling Utilities",
      "description": "Create standardized error handling utilities to consolidate repeated try-catch-log patterns across the codebase.",
      "details": "Create `src/pytest_analyzer/core/cross_cutting/error_handling.py` with the following implementation:\n\n```python\nfrom contextlib import contextmanager\nfrom typing import Optional, Type, TypeVar, Callable, Any, Union, List\nimport logging\nimport functools\nimport traceback\nimport sys\n\n# Import from the new structure\nfrom pytest_analyzer.core.interfaces.errors import BaseError\n\nT = TypeVar('T')\nE = TypeVar('E', bound=BaseError)\n\n@contextmanager\ndef error_context(operation_name: str, logger: Optional[logging.Logger] = None, \n                 error_type: Optional[Type[E]] = None, reraise: bool = True):\n    \"\"\"Standard error context manager for consistent error handling.\n    \n    Args:\n        operation_name: Name of the operation being performed for logging\n        logger: Logger instance to use (creates one if not provided)\n        error_type: Custom error type to raise instead of the original\n        reraise: Whether to re-raise the exception (True) or suppress it (False)\n    \"\"\"\n    logger = logger or logging.getLogger(\"error_context\")\n    try:\n        yield\n    except Exception as e:\n        exc_info = sys.exc_info()\n        logger.error(f\"{operation_name} failed: {str(e)}\")\n        logger.debug(f\"Exception details: {traceback.format_exception(*exc_info)}\")\n        \n        if reraise:\n            if error_type:\n                raise error_type(f\"{operation_name}: {str(e)}\") from e\n            raise\n\ndef error_handler(operation_name: str, error_type: Optional[Type[E]] = None, \n                 reraise: bool = True, logger: Optional[logging.Logger] = None):\n    \"\"\"Decorator for consistent error handling across functions.\n    \n    Args:\n        operation_name: Name of the operation being performed for logging\n        error_type: Custom error type to raise instead of the original\n        reraise: Whether to re-raise the exception (True) or suppress it (False)\n        logger: Logger instance to use (creates one if not provided)\n    \"\"\"\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            nonlocal logger\n            logger = logger or logging.getLogger(func.__module__)\n            with error_context(operation_name, logger, error_type, reraise):\n                return func(*args, **kwargs)\n        return wrapper\n    return decorator\n\ndef batch_operation(items: List[T], operation: Callable[[T], Any], \n                   operation_name: str, continue_on_error: bool = False,\n                   logger: Optional[logging.Logger] = None) -> List[Any]:\n    \"\"\"Process a batch of items with consistent error handling.\n    \n    Args:\n        items: List of items to process\n        operation: Function to apply to each item\n        operation_name: Name of the operation for logging\n        continue_on_error: Whether to continue processing after errors\n        logger: Logger instance to use\n    \n    Returns:\n        List of results from successful operations\n    \"\"\"\n    logger = logger or logging.getLogger(\"batch_operation\")\n    results = []\n    \n    for i, item in enumerate(items):\n        try:\n            result = operation(item)\n            results.append(result)\n        except Exception as e:\n            logger.error(f\"{operation_name} failed for item {i}: {str(e)}\")\n            if not continue_on_error:\n                raise\n    \n    return results\n```\n\nThis module provides standardized error handling utilities that can be used across the codebase to replace repeated try-catch-log patterns.",
      "testStrategy": "Create unit tests for error handling utilities that verify:\n1. error_context correctly logs exceptions and re-raises them\n2. error_context with reraise=False suppresses exceptions\n3. error_context with custom error_type wraps exceptions in the specified type\n4. error_handler decorator properly wraps functions with error handling\n5. batch_operation correctly processes items and handles errors according to continue_on_error",
      "priority": "medium",
      "dependencies": [
        1
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 6,
      "title": "Migrate PoetryEnvironmentManager to Use Base Class",
      "description": "Refactor the PoetryEnvironmentManager to inherit from BaseEnvironmentManager and eliminate duplicated code.",
      "details": "Refactor `src/pytest_analyzer/core/infrastructure/environment/poetry_manager.py` to use the new base class:\n\n```python\nfrom pathlib import Path\nfrom typing import List\n\n# Import from the new structure\nfrom pytest_analyzer.core.infrastructure.environment.base_manager import BaseEnvironmentManager\nfrom pytest_analyzer.core.cross_cutting.error_handling import error_context\n\nclass PoetryEnvironmentManager(BaseEnvironmentManager):\n    \"\"\"Poetry-specific environment manager implementation.\"\"\"\n    \n    def __init__(self, project_path: Path):\n        super().__init__(project_path)\n        self.poetry_executable = \"poetry\"\n    \n    def build_command(self, base_command: List[str]) -> List[str]:\n        \"\"\"Build a Poetry-specific command to run the given base command.\"\"\"\n        with error_context(\"Building Poetry command\", self.logger):\n            return [self.poetry_executable, \"run\"] + base_command\n    \n    # Only implement methods that differ from the base class\n    # activate and deactivate can use the base class no-op implementations\n    # execute_command can use the base class implementation\n```\n\nRepeat this pattern for other environment managers (Pixi, Hatch, UV, Pipenv) to eliminate code duplication while preserving specific behavior for each manager.",
      "testStrategy": "Create unit tests for PoetryEnvironmentManager that verify:\n1. build_command correctly prepends 'poetry run' to commands\n2. Inherited methods from BaseEnvironmentManager work correctly\n3. Integration test that verifies the manager can execute real commands in a Poetry environment\n4. Error handling works correctly when invalid commands are provided",
      "priority": "medium",
      "dependencies": [
        2
      ],
      "status": "in-progress",
      "subtasks": []
    },
    {
      "id": 7,
      "title": "Migrate Remaining Environment Managers to Base Class",
      "description": "Refactor Pixi, Hatch, UV, and Pipenv environment managers to inherit from BaseEnvironmentManager.",
      "details": "Refactor the remaining environment managers following the same pattern as PoetryEnvironmentManager:\n\n1. For PixiEnvironmentManager:\n```python\nfrom pathlib import Path\nfrom typing import List\n\nfrom pytest_analyzer.core.infrastructure.environment.base_manager import BaseEnvironmentManager\nfrom pytest_analyzer.core.cross_cutting.error_handling import error_context\n\nclass PixiEnvironmentManager(BaseEnvironmentManager):\n    \"\"\"Pixi-specific environment manager implementation.\"\"\"\n    \n    def __init__(self, project_path: Path):\n        super().__init__(project_path)\n        self.pixi_executable = \"pixi\"\n    \n    def build_command(self, base_command: List[str]) -> List[str]:\n        \"\"\"Build a Pixi-specific command to run the given base command.\"\"\"\n        with error_context(\"Building Pixi command\", self.logger):\n            return [self.pixi_executable, \"run\"] + base_command\n```\n\n2. For HatchEnvironmentManager:\n```python\nfrom pathlib import Path\nfrom typing import List\n\nfrom pytest_analyzer.core.infrastructure.environment.base_manager import BaseEnvironmentManager\nfrom pytest_analyzer.core.cross_cutting.error_handling import error_context\n\nclass HatchEnvironmentManager(BaseEnvironmentManager):\n    \"\"\"Hatch-specific environment manager implementation.\"\"\"\n    \n    def __init__(self, project_path: Path):\n        super().__init__(project_path)\n        self.hatch_executable = \"hatch\"\n    \n    def build_command(self, base_command: List[str]) -> List[str]:\n        \"\"\"Build a Hatch-specific command to run the given base command.\"\"\"\n        with error_context(\"Building Hatch command\", self.logger):\n            return [self.hatch_executable, \"run\"] + base_command\n```\n\n3. For UVEnvironmentManager and PipenvEnvironmentManager, follow the same pattern.\n\nEnsure each manager only implements the methods that differ from the base class implementation.",
      "testStrategy": "For each environment manager, create unit tests that verify:\n1. build_command correctly prepends the appropriate executable and commands\n2. Inherited methods from BaseEnvironmentManager work correctly\n3. Integration test that verifies the manager can execute real commands in the appropriate environment\n4. Error handling works correctly when invalid commands are provided\n5. Verify that the refactored managers maintain the same behavior as the original implementations",
      "priority": "medium",
      "dependencies": [
        2,
        6
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 8,
      "title": "Implement EnvironmentManagerFactory Using BaseFactory",
      "description": "Create an EnvironmentManagerFactory that inherits from BaseFactory to create the appropriate environment manager based on project configuration.",
      "details": "Create `src/pytest_analyzer/core/infrastructure/environment/environment_manager_factory.py` with the following implementation:\n\n```python\nfrom pathlib import Path\nfrom typing import Type, Optional, Dict\n\nfrom pytest_analyzer.core.infrastructure.base_factory import BaseFactory\nfrom pytest_analyzer.core.infrastructure.environment.base_manager import BaseEnvironmentManager\nfrom pytest_analyzer.core.infrastructure.environment.poetry_manager import PoetryEnvironmentManager\nfrom pytest_analyzer.core.infrastructure.environment.pixi_manager import PixiEnvironmentManager\nfrom pytest_analyzer.core.infrastructure.environment.hatch_manager import HatchEnvironmentManager\nfrom pytest_analyzer.core.infrastructure.environment.uv_manager import UVEnvironmentManager\nfrom pytest_analyzer.core.infrastructure.environment.pipenv_manager import PipenvEnvironmentManager\nfrom pytest_analyzer.core.cross_cutting.configuration.settings import Settings\nfrom pytest_analyzer.core.cross_cutting.error_handling import error_context\n\nclass EnvironmentManagerFactory(BaseFactory):\n    \"\"\"Factory for creating environment managers based on project configuration.\"\"\"\n    \n    def __init__(self, settings: Optional[Settings] = None):\n        super().__init__(settings)\n        self._register_default_managers()\n    \n    def _register_default_managers(self) -> None:\n        \"\"\"Register the default environment managers.\"\"\"\n        self.register(\"poetry\", PoetryEnvironmentManager)\n        self.register(\"pixi\", PixiEnvironmentManager)\n        self.register(\"hatch\", HatchEnvironmentManager)\n        self.register(\"uv\", UVEnvironmentManager)\n        self.register(\"pipenv\", PipenvEnvironmentManager)\n    \n    def create(self, project_path: Path, manager_type: Optional[str] = None) -> BaseEnvironmentManager:\n        \"\"\"Create an environment manager for the given project path.\n        \n        Args:\n            project_path: Path to the project directory\n            manager_type: Type of manager to create, or None to auto-detect\n            \n        Returns:\n            An instance of the appropriate environment manager\n        \"\"\"\n        with error_context(\"Creating environment manager\", self.logger):\n            if manager_type is None:\n                manager_type = self._detect_environment_type(project_path)\n            \n            manager_class = self.get_implementation(manager_type)\n            return manager_class(project_path)\n    \n    def _detect_environment_type(self, project_path: Path) -> str:\n        \"\"\"Detect the environment type based on project files.\"\"\"\n        detection_files = {\n            \"poetry\": \"pyproject.toml\",  # Look for poetry section\n            \"pixi\": \"pixi.toml\",\n            \"hatch\": \"pyproject.toml\",  # Look for hatch section\n            \"pipenv\": \"Pipfile\",\n            \"uv\": \"pyproject.toml\"  # Look for uv section\n        }\n        \n        # Check for pyproject.toml first and determine the tool\n        if (project_path / \"pyproject.toml\").exists():\n            # Read the file and check for tool sections\n            with open(project_path / \"pyproject.toml\", \"r\") as f:\n                content = f.read()\n                if \"[tool.poetry]\" in content:\n                    return \"poetry\"\n                elif \"[tool.hatch]\" in content:\n                    return \"hatch\"\n                elif \"[tool.uv]\" in content:\n                    return \"uv\"\n        \n        # Check for other files\n        for env_type, filename in detection_files.items():\n            if (project_path / filename).exists():\n                return env_type\n        \n        # Default to poetry if nothing else is detected\n        self.logger.warning(f\"Could not detect environment type for {project_path}, defaulting to poetry\")\n        return \"poetry\"\n```\n\nThis factory uses the BaseFactory to eliminate code duplication while providing environment manager-specific functionality.",
      "testStrategy": "Create unit tests for EnvironmentManagerFactory that verify:\n1. _register_default_managers correctly registers all environment managers\n2. create returns the correct manager type when explicitly specified\n3. _detect_environment_type correctly identifies environment types based on project files\n4. Integration test with different project structures to verify auto-detection works\n5. Error handling for invalid manager types",
      "priority": "medium",
      "dependencies": [
        3,
        6,
        7
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 9,
      "title": "Migrate ExtractorFactory to Use BaseFactory",
      "description": "Refactor the ExtractorFactory to inherit from BaseFactory and eliminate duplicated code.",
      "details": "Refactor `src/pytest_analyzer/core/infrastructure/extraction/extractor_factory.py` to use the new base class:\n\n```python\nfrom typing import Type, Optional\nfrom pathlib import Path\n\nfrom pytest_analyzer.core.infrastructure.base_factory import BaseFactory\nfrom pytest_analyzer.core.interfaces.protocols import Extractor\nfrom pytest_analyzer.core.infrastructure.extraction.json_extractor import JSONExtractor\nfrom pytest_analyzer.core.infrastructure.extraction.xml_extractor import XMLExtractor\nfrom pytest_analyzer.core.infrastructure.extraction.text_extractor import TextExtractor\nfrom pytest_analyzer.core.cross_cutting.configuration.settings import Settings\nfrom pytest_analyzer.core.cross_cutting.error_handling import error_context\n\nclass ExtractorFactory(BaseFactory):\n    \"\"\"Factory for creating extractors based on file type.\"\"\"\n    \n    def __init__(self, settings: Optional[Settings] = None):\n        super().__init__(settings)\n        self._register_default_extractors()\n    \n    def _register_default_extractors(self) -> None:\n        \"\"\"Register the default extractors.\"\"\"\n        self.register(\"json\", JSONExtractor)\n        self.register(\"xml\", XMLExtractor)\n        self.register(\"txt\", TextExtractor)\n    \n    def create(self, file_path: str) -> Extractor:\n        \"\"\"Create an extractor for the given file path.\n        \n        Args:\n            file_path: Path to the file to extract from\n            \n        Returns:\n            An instance of the appropriate extractor\n        \"\"\"\n        with error_context(\"Creating extractor\", self.logger):\n            file_type = self._detect_file_type(file_path)\n            extractor_class = self.get_implementation(file_type)\n            return extractor_class()\n```\n\nThis refactored factory uses the BaseFactory to eliminate code duplication while providing extractor-specific functionality.",
      "testStrategy": "Create unit tests for ExtractorFactory that verify:\n1. _register_default_extractors correctly registers all extractors\n2. create returns the correct extractor type based on file extension\n3. Integration test with different file types to verify correct extractor is created\n4. Error handling for unsupported file types\n5. Verify that the refactored factory maintains the same behavior as the original implementation",
      "priority": "medium",
      "dependencies": [
        3
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 10,
      "title": "Migrate LLMServiceFactory to Use BaseFactory",
      "description": "Refactor the LLMServiceFactory to inherit from BaseFactory and eliminate duplicated code.",
      "details": "Refactor `src/pytest_analyzer/core/infrastructure/llm/llm_service_factory.py` to use the new base class:\n\n```python\nfrom typing import Type, Optional, Dict, Any\n\nfrom pytest_analyzer.core.infrastructure.base_factory import BaseFactory\nfrom pytest_analyzer.core.interfaces.protocols import LLMService, LLMProvider\nfrom pytest_analyzer.core.infrastructure.llm.openai_service import OpenAIService\nfrom pytest_analyzer.core.infrastructure.llm.anthropic_service import AnthropicService\nfrom pytest_analyzer.core.infrastructure.llm.mock_service import MockLLMService\nfrom pytest_analyzer.core.cross_cutting.configuration.settings import Settings\nfrom pytest_analyzer.core.cross_cutting.error_handling import error_context\n\nclass LLMServiceFactory(BaseFactory):\n    \"\"\"Factory for creating LLM services based on configuration.\"\"\"\n    \n    def __init__(self, settings: Optional[Settings] = None):\n        super().__init__(settings)\n        self._register_default_services()\n    \n    def _register_default_services(self) -> None:\n        \"\"\"Register the default LLM services.\"\"\"\n        self.register(\"openai\", OpenAIService)\n        self.register(\"anthropic\", AnthropicService)\n        self.register(\"mock\", MockLLMService)\n    \n    def create(self, provider_type: Optional[str] = None, provider: Optional[LLMProvider] = None) -> LLMService:\n        \"\"\"Create an LLM service with the specified provider.\n        \n        Args:\n            provider_type: Type of LLM provider to use, or None to use settings\n            provider: Optional pre-configured provider instance\n            \n        Returns:\n            An instance of the appropriate LLM service\n        \"\"\"\n        with error_context(\"Creating LLM service\", self.logger):\n            if provider_type is None:\n                provider_type = self.settings.get(\"llm.provider\", \"openai\")\n            \n            service_class = self.get_implementation(provider_type)\n            return service_class(provider=provider, settings=self.settings)\n```\n\nThis refactored factory uses the BaseFactory to eliminate code duplication while providing LLM service-specific functionality.",
      "testStrategy": "Create unit tests for LLMServiceFactory that verify:\n1. _register_default_services correctly registers all LLM services\n2. create returns the correct service type based on provider_type\n3. create uses settings to determine provider_type when not specified\n4. create correctly passes provider and settings to the service constructor\n5. Error handling for unsupported provider types\n6. Verify that the refactored factory maintains the same behavior as the original implementation",
      "priority": "medium",
      "dependencies": [
        3,
        4
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 11,
      "title": "Refactor Sync and Async LLM Services to Use BaseLLMService",
      "description": "Refactor the synchronous and asynchronous LLM service implementations to inherit from BaseLLMService and eliminate duplicated code.",
      "details": "Refactor the LLM service implementations to use the new base class:\n\n1. For synchronous LLM service (`src/pytest_analyzer/core/infrastructure/llm/llm_service.py`):\n```python\nfrom typing import Dict, Any, Optional\n\nfrom pytest_analyzer.core.infrastructure.llm.base_llm_service import BaseLLMService\nfrom pytest_analyzer.core.interfaces.protocols import LLMProvider\nfrom pytest_analyzer.core.cross_cutting.configuration.settings import Settings\nfrom pytest_analyzer.core.cross_cutting.error_handling import error_context\n\nclass LLMService(BaseLLMService):\n    \"\"\"Synchronous LLM service implementation.\"\"\"\n    \n    def __init__(self, provider: Optional[LLMProvider] = None, settings: Optional[Settings] = None):\n        super().__init__(provider, settings)\n    \n    def _create_default_provider(self) -> LLMProvider:\n        \"\"\"Create the default LLM provider based on settings.\"\"\"\n        provider_name = self.settings.get(\"llm.provider\", \"openai\")\n        \n        if provider_name == \"openai\":\n            from pytest_analyzer.core.infrastructure.llm.providers.openai_provider import OpenAIProvider\n            return OpenAIProvider(self.settings)\n        elif provider_name == \"anthropic\":\n            from pytest_analyzer.core.infrastructure.llm.providers.anthropic_provider import AnthropicProvider\n            return AnthropicProvider(self.settings)\n        else:\n            raise ValueError(f\"Unsupported LLM provider: {provider_name}\")\n    \n    def generate(self, prompt: str, context: Optional[Dict[str, Any]] = None) -> str:\n        \"\"\"Generate a response from the LLM based on the prompt and context.\"\"\"\n        with error_context(\"Generating LLM response\", self.logger):\n            messages = self._prepare_messages(prompt, context)\n            response = self.provider.complete(\n                messages=messages,\n                model=self.model,\n                temperature=self.temperature,\n                max_tokens=self.max_tokens,\n                timeout=self.timeout\n            )\n            return response\n```\n\n2. For asynchronous LLM service (`src/pytest_analyzer/core/infrastructure/llm/async_llm_service.py`):\n```python\nfrom typing import Dict, Any, Optional\nimport asyncio\n\nfrom pytest_analyzer.core.infrastructure.llm.base_llm_service import BaseLLMService\nfrom pytest_analyzer.core.interfaces.protocols import AsyncLLMProvider, LLMProvider\nfrom pytest_analyzer.core.cross_cutting.configuration.settings import Settings\nfrom pytest_analyzer.core.cross_cutting.error_handling import error_context\n\nclass AsyncLLMService(BaseLLMService):\n    \"\"\"Asynchronous LLM service implementation.\"\"\"\n    \n    def __init__(self, provider: Optional[AsyncLLMProvider] = None, settings: Optional[Settings] = None):\n        super().__init__(provider, settings)\n    \n    def _create_default_provider(self) -> AsyncLLMProvider:\n        \"\"\"Create the default async LLM provider based on settings.\"\"\"\n        provider_name = self.settings.get(\"llm.provider\", \"openai\")\n        \n        if provider_name == \"openai\":\n            from pytest_analyzer.core.infrastructure.llm.providers.async_openai_provider import AsyncOpenAIProvider\n            return AsyncOpenAIProvider(self.settings)\n        elif provider_name == \"anthropic\":\n            from pytest_analyzer.core.infrastructure.llm.providers.async_anthropic_provider import AsyncAnthropicProvider\n            return AsyncAnthropicProvider(self.settings)\n        else:\n            raise ValueError(f\"Unsupported async LLM provider: {provider_name}\")\n    \n    async def generate_async(self, prompt: str, context: Optional[Dict[str, Any]] = None) -> str:\n        \"\"\"Generate a response asynchronously from the LLM based on the prompt and context.\"\"\"\n        with error_context(\"Generating async LLM response\", self.logger):\n            messages = self._prepare_messages(prompt, context)\n            response = await self.provider.complete_async(\n                messages=messages,\n                model=self.model,\n                temperature=self.temperature,\n                max_tokens=self.max_tokens,\n                timeout=self.timeout\n            )\n            return response\n    \n    def generate(self, prompt: str, context: Optional[Dict[str, Any]] = None) -> str:\n        \"\"\"Synchronous wrapper for generate_async.\"\"\"\n        return asyncio.run(self.generate_async(prompt, context))\n```\n\nThese refactored services use the BaseLLMService to eliminate code duplication while providing synchronous and asynchronous-specific functionality.",
      "testStrategy": "Create unit tests for both LLM services that verify:\n1. _create_default_provider correctly creates the appropriate provider\n2. generate (and generate_async) correctly call the provider's complete method\n3. Error handling for provider errors\n4. Verify that the refactored services maintain the same behavior as the original implementations\n5. Test with mock providers to ensure correct parameter passing",
      "priority": "medium",
      "dependencies": [
        4,
        10
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 12,
      "title": "Remove Example Files and Unused Code",
      "description": "Remove example files and unused code patterns identified in the PRD to reduce codebase size and complexity.",
      "details": "Remove the following files and code patterns:\n\n1. Example files:\n   - `src/pytest_analyzer/core/di/examples.py` (381 lines)\n   - `src/pytest_analyzer/core/state_machine/example.py` (194 lines)\n\n2. Unused decorators:\n   - Identify and remove unused DI decorators\n   - Remove over-complex registration modes that aren't used\n\n3. Process for removal:\n   - Create a backup of each file before removal\n   - Update any import statements that reference these files\n   - Remove the files from the codebase\n   - Run the test suite to ensure no regressions\n\n4. Documentation updates:\n   - Update any documentation that references these files\n   - Add comments explaining the removal in relevant locations\n\nThis task should result in a reduction of approximately 575 lines of code that serve no production purpose.",
      "testStrategy": "1. Verify that all tests pass after removal of example files\n2. Check for any import errors related to the removed files\n3. Verify that the codebase size has been reduced by the expected amount\n4. Run static analysis tools to ensure no new issues are introduced\n5. Verify that documentation is updated to reflect the changes",
      "priority": "low",
      "dependencies": [
        1
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 13,
      "title": "Implement Domain-Driven Design Entity Classes",
      "description": "Create core domain entity classes following DDD principles to represent the core business objects of the system.",
      "details": "Create the following domain entity classes in the `src/pytest_analyzer/core/domain/entities` directory:\n\n1. `pytest_failure.py`:\n```python\nfrom dataclasses import dataclass\nfrom typing import List, Optional\nfrom pathlib import Path\n\nfrom pytest_analyzer.core.domain.value_objects.test_location import TestLocation\nfrom pytest_analyzer.core.domain.value_objects.failure_type import FailureType\n\n@dataclass\nclass PytestFailure:\n    \"\"\"Represents a test failure from a pytest execution.\"\"\"\n    id: str\n    test_name: str\n    location: TestLocation\n    failure_message: str\n    failure_type: FailureType\n    traceback: List[str]\n    source_code: Optional[str] = None\n    \n    @property\n    def is_assertion_error(self) -> bool:\n        \"\"\"Check if this failure is an assertion error.\"\"\"\n        return self.failure_type == FailureType.ASSERTION_ERROR\n    \n    @property\n    def is_exception(self) -> bool:\n        \"\"\"Check if this failure is an exception.\"\"\"\n        return self.failure_type == FailureType.EXCEPTION\n    \n    @property\n    def file_path(self) -> Path:\n        \"\"\"Get the file path of the test.\"\"\"\n        return self.location.file_path\n```\n\n2. `fix_suggestion.py`:\n```python\nfrom dataclasses import dataclass\nfrom typing import List, Optional\nfrom enum import Enum\n\nclass SuggestionConfidence(Enum):\n    HIGH = \"high\"\n    MEDIUM = \"medium\"\n    LOW = \"low\"\n\n@dataclass\nclass FixSuggestion:\n    \"\"\"Represents a suggested fix for a test failure.\"\"\"\n    failure_id: str\n    suggestion_text: str\n    code_changes: List[str]\n    confidence: SuggestionConfidence\n    explanation: str\n    alternative_approaches: Optional[List[str]] = None\n```\n\n3. Create appropriate value objects in `src/pytest_analyzer/core/domain/value_objects/`:\n   - `test_location.py` for TestLocation\n   - `failure_type.py` for FailureType\n\nThese domain entities represent the core business objects of the system and follow DDD principles.",
      "testStrategy": "Create unit tests for each domain entity that verify:\n1. PytestFailure properties work correctly\n2. FixSuggestion can be created with all required fields\n3. Value objects are immutable and correctly represent their concepts\n4. Entity equality is based on identity (id field)\n5. Test serialization/deserialization of entities",
      "priority": "high",
      "dependencies": [
        1
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 14,
      "title": "Implement Domain Service Interfaces and Base Implementations",
      "description": "Create domain service interfaces and base implementations following DDD principles to define the core business logic of the system.",
      "details": "Create the following domain service interfaces and base implementations:\n\n1. `src/pytest_analyzer/core/interfaces/protocols.py` (domain service interfaces):\n```python\nfrom typing import Protocol, List, Optional, Dict, Any\nfrom pathlib import Path\n\nfrom pytest_analyzer.core.domain.entities.pytest_failure import PytestFailure\nfrom pytest_analyzer.core.domain.entities.fix_suggestion import FixSuggestion\n\nclass FailureAnalyzer(Protocol):\n    \"\"\"Protocol for analyzing test failures.\"\"\"\n    \n    def analyze(self, failure: PytestFailure) -> Dict[str, Any]:\n        \"\"\"Analyze a test failure and return analysis results.\"\"\"\n        ...\n\nclass FixSuggester(Protocol):\n    \"\"\"Protocol for suggesting fixes for test failures.\"\"\"\n    \n    def suggest(self, failure: PytestFailure, analysis: Optional[Dict[str, Any]] = None) -> FixSuggestion:\n        \"\"\"Suggest a fix for a test failure.\"\"\"\n        ...\n\nclass TestResultRepository(Protocol):\n    \"\"\"Protocol for accessing test results.\"\"\"\n    \n    def get_failures(self, report_path: Path) -> List[PytestFailure]:\n        \"\"\"Get all failures from a test report.\"\"\"\n        ...\n    \n    def save_suggestions(self, suggestions: List[FixSuggestion], output_path: Path) -> None:\n        \"\"\"Save fix suggestions to an output file.\"\"\"\n        ...\n```\n\n2. `src/pytest_analyzer/core/domain/services/base_failure_analyzer.py`:\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Any\n\nfrom pytest_analyzer.core.domain.entities.pytest_failure import PytestFailure\nfrom pytest_analyzer.core.cross_cutting.error_handling import error_handler\n\nclass BaseFailureAnalyzer(ABC):\n    \"\"\"Base class for failure analyzers.\"\"\"\n    \n    @error_handler(\"Analyzing test failure\")\n    def analyze(self, failure: PytestFailure) -> Dict[str, Any]:\n        \"\"\"Analyze a test failure and return analysis results.\"\"\"\n        # Common pre-processing logic\n        result = self._perform_analysis(failure)\n        # Common post-processing logic\n        return result\n    \n    @abstractmethod\n    def _perform_analysis(self, failure: PytestFailure) -> Dict[str, Any]:\n        \"\"\"Perform the actual analysis (to be implemented by subclasses).\"\"\"\n        pass\n```\n\n3. `src/pytest_analyzer/core/domain/services/base_fix_suggester.py`:\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Any, Optional\n\nfrom pytest_analyzer.core.domain.entities.pytest_failure import PytestFailure\nfrom pytest_analyzer.core.domain.entities.fix_suggestion import FixSuggestion\nfrom pytest_analyzer.core.cross_cutting.error_handling import error_handler\n\nclass BaseFixSuggester(ABC):\n    \"\"\"Base class for fix suggesters.\"\"\"\n    \n    @error_handler(\"Suggesting fix for test failure\")\n    def suggest(self, failure: PytestFailure, analysis: Optional[Dict[str, Any]] = None) -> FixSuggestion:\n        \"\"\"Suggest a fix for a test failure.\"\"\"\n        # Common pre-processing logic\n        suggestion = self._generate_suggestion(failure, analysis or {})\n        # Common post-processing logic\n        return suggestion\n    \n    @abstractmethod\n    def _generate_suggestion(self, failure: PytestFailure, analysis: Dict[str, Any]) -> FixSuggestion:\n        \"\"\"Generate the actual suggestion (to be implemented by subclasses).\"\"\"\n        pass\n```\n\nThese interfaces and base implementations define the core business logic of the system following DDD principles.",
      "testStrategy": "Create unit tests for the base domain services that verify:\n1. BaseFailureAnalyzer correctly calls _perform_analysis\n2. BaseFixSuggester correctly calls _generate_suggestion\n3. Error handling works correctly in both base classes\n4. Protocol definitions can be used for type checking\n5. Create mock implementations to verify protocol compatibility",
      "priority": "high",
      "dependencies": [
        1,
        13
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 15,
      "title": "Implement Application Service Facade",
      "description": "Create an application service facade that provides a clean, high-level API for the pytest-analyzer functionality.",
      "details": "Create an application service facade in `src/pytest_analyzer/core/application/services/analyzer_service.py`:\n\n```python\nfrom pathlib import Path\nfrom typing import List, Optional, Dict, Any\nimport logging\n\nfrom pytest_analyzer.core.domain.entities.pytest_failure import PytestFailure\nfrom pytest_analyzer.core.domain.entities.fix_suggestion import FixSuggestion\nfrom pytest_analyzer.core.interfaces.protocols import FailureAnalyzer, FixSuggester, TestResultRepository\nfrom pytest_analyzer.core.cross_cutting.error_handling import error_handler, batch_operation\nfrom pytest_analyzer.core.cross_cutting.configuration.settings import Settings\n\nclass AnalyzerService:\n    \"\"\"Application service facade for the pytest-analyzer functionality.\"\"\"\n    \n    def __init__(self, \n                 repository: TestResultRepository,\n                 analyzer: FailureAnalyzer,\n                 suggester: FixSuggester,\n                 settings: Optional[Settings] = None):\n        self.repository = repository\n        self.analyzer = analyzer\n        self.suggester = suggester\n        self.settings = settings or Settings()\n        self.logger = logging.getLogger(self.__class__.__name__)\n    \n    @error_handler(\"Analyzing test report\")\n    def analyze_report(self, report_path: Path, output_path: Optional[Path] = None) -> List[FixSuggestion]:\n        \"\"\"Analyze a test report and generate fix suggestions.\n        \n        Args:\n            report_path: Path to the test report file\n            output_path: Optional path to save suggestions to\n            \n        Returns:\n            List of fix suggestions\n        \"\"\"\n        # Get failures from the repository\n        failures = self.repository.get_failures(report_path)\n        self.logger.info(f\"Found {len(failures)} failures in {report_path}\")\n        \n        if not failures:\n            self.logger.info(\"No failures to analyze\")\n            return []\n        \n        # Process each failure\n        suggestions = self._process_failures(failures)\n        \n        # Save suggestions if output path is provided\n        if output_path and suggestions:\n            self.repository.save_suggestions(suggestions, output_path)\n            self.logger.info(f\"Saved {len(suggestions)} suggestions to {output_path}\")\n        \n        return suggestions\n    \n    def _process_failures(self, failures: List[PytestFailure]) -> List[FixSuggestion]:\n        \"\"\"Process a list of failures to generate suggestions.\"\"\"\n        # First analyze all failures\n        analyses = {}\n        for failure in failures:\n            try:\n                analyses[failure.id] = self.analyzer.analyze(failure)\n            except Exception as e:\n                self.logger.error(f\"Error analyzing failure {failure.id}: {str(e)}\")\n                analyses[failure.id] = {}\n        \n        # Then generate suggestions for each failure\n        continue_on_error = self.settings.get(\"analysis.continue_on_error\", \"true\").lower() == \"true\"\n        \n        def suggest_fix(failure: PytestFailure) -> FixSuggestion:\n            return self.suggester.suggest(failure, analyses.get(failure.id))\n        \n        return batch_operation(\n            items=failures,\n            operation=suggest_fix,\n            operation_name=\"Suggesting fix\",\n            continue_on_error=continue_on_error,\n            logger=self.logger\n        )\n```\n\nThis application service facade provides a clean, high-level API for the pytest-analyzer functionality and follows DDD principles.",
      "testStrategy": "Create unit tests for the AnalyzerService that verify:\n1. analyze_report correctly processes failures and returns suggestions\n2. _process_failures correctly handles errors based on settings\n3. Integration test with mock dependencies to verify end-to-end flow\n4. Error handling for repository, analyzer, and suggester errors\n5. Verify that suggestions are saved when output_path is provided",
      "priority": "high",
      "dependencies": [
        1,
        13,
        14
      ],
      "status": "done",
      "subtasks": []
    }
  ]
}
