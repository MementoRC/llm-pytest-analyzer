{
  "tasks": [
    {
      "id": 1,
      "title": "Create MCP Module Structure",
      "description": "Create the foundational module structure for the MCP server integration within the existing project architecture.",
      "details": "Create the following directory and file structure:\n- `src/pytest_analyzer/mcp/` (main module directory)\n- `src/pytest_analyzer/mcp/__init__.py` (module initialization)\n- `src/pytest_analyzer/mcp/server.py` (MCP server implementation)\n- `src/pytest_analyzer/mcp/tools/` (directory for tool implementations)\n- `src/pytest_analyzer/mcp/schemas/` (directory for Pydantic schemas)\n- `src/pytest_analyzer/mcp/resources/` (directory for MCP resources)\n- `src/pytest_analyzer/mcp/prompts/` (directory for LLM prompts)\n\nEnsure the module structure follows existing project patterns and coding standards. Update package imports and exports as needed.",
      "testStrategy": "Verify directory structure exists and is importable. Ensure module initialization works correctly. Check that the structure follows project conventions through code review.",
      "priority": "high",
      "dependencies": [],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 2,
      "title": "Add MCP SDK Dependencies",
      "description": "Integrate the MCP Python SDK as a project dependency with proper version management in pyproject.toml.",
      "details": "Update `pyproject.toml` to include:\n1. Add MCP Python SDK as an optional dependency\n2. Create a new optional dependency group for MCP server functionality\n3. Set appropriate version constraints\n4. Ensure no conflicts with existing dependencies\n5. Update development dependencies for testing MCP functionality\n\nExample addition to pyproject.toml:\n```toml\n[project.optional-dependencies]\nmcp = [\n    \"mcp-python-sdk>=1.0.0,<2.0.0\",\n    \"fastapi>=0.95.0,<1.0.0\",  # For HTTP transport\n    \"uvicorn>=0.22.0,<1.0.0\",  # For HTTP transport\n]\n```\n\nRun dependency resolution tests to ensure compatibility.",
      "testStrategy": "Verify dependency installation with `pip install -e \".[mcp]\"`. Test importing MCP SDK in Python. Check for any dependency conflicts or warnings. Ensure development environment can be set up with new dependencies.",
      "priority": "high",
      "dependencies": [
        1
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 3,
      "title": "Implement MCP Server Base Class",
      "description": "Create the core MCP server implementation that will handle protocol communication and tool registration.",
      "details": "In `src/pytest_analyzer/mcp/server.py`, implement:\n1. `MCPServer` class that initializes the MCP protocol handler\n2. Methods for registering tools and resources\n3. Transport configuration (STDIO and HTTP)\n4. Server lifecycle management (start, stop)\n5. Error handling and logging integration\n\nImplementation should:\n- Use the MCP Python SDK for protocol handling\n- Support both STDIO and HTTP transports\n- Integrate with existing logging system\n- Handle graceful shutdown\n- Support dependency injection pattern\n\nExample structure:\n```python\nfrom mcp_python_sdk import MCPProtocolHandler, Transport\nfrom pytest_analyzer.core.logging import get_logger\n\nclass MCPServer:\n    def __init__(self, transport_type=\"stdio\", host=\"127.0.0.1\", port=8000):\n        self.logger = get_logger(__name__)\n        self.transport_type = transport_type\n        self.host = host\n        self.port = port\n        self.protocol_handler = MCPProtocolHandler()\n        self.tools = {}\n        \n    def register_tool(self, tool_name, tool_function, schema):\n        # Implementation\n        \n    def start(self):\n        # Implementation\n        \n    def stop(self):\n        # Implementation\n```",
      "testStrategy": "Unit test the MCPServer class with mock protocol handler. Test server initialization with different transport configurations. Verify tool registration works correctly. Test server start/stop lifecycle. Verify error handling for invalid configurations.",
      "priority": "high",
      "dependencies": [
        1,
        2
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 4,
      "title": "Implement CLI Integration for MCP Server",
      "description": "Extend the existing CLI interface to support starting and configuring the MCP server.",
      "details": "Extend the CLI module to add MCP server commands:\n1. Add `mcp-server` subcommand to the main CLI entrypoint\n2. Implement options for transport selection (--stdio, --http)\n3. Add port and host configuration for HTTP transport\n4. Include help text and documentation\n5. Implement graceful shutdown handling\n\nExample implementation:\n```python\n@click.group(name=\"mcp-server\")\ndef mcp_server_group():\n    \"\"\"Start and manage the MCP server for AI assistant integration.\"\"\"\n    pass\n\n@mcp_server_group.command(name=\"start\")\n@click.option(\"--stdio\", is_flag=True, help=\"Use STDIO transport (default)\")\n@click.option(\"--http\", is_flag=True, help=\"Use HTTP transport\")\n@click.option(\"--port\", default=8000, help=\"Port for HTTP transport\")\n@click.option(\"--host\", default=\"127.0.0.1\", help=\"Host for HTTP transport\")\ndef start_server(stdio, http, port, host):\n    \"\"\"Start the MCP server with specified transport.\"\"\"\n    # Implementation\n    \n# Register with main CLI\nmain_cli.add_command(mcp_server_group)\n```\n\nEnsure proper signal handling for graceful shutdown (SIGINT, SIGTERM).",
      "testStrategy": "Test CLI command registration and help text. Verify command-line arguments are correctly parsed. Test server startup with different transport options. Check signal handling for proper shutdown. Integration test with actual server instance.",
      "priority": "high",
      "dependencies": [
        3
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 5,
      "title": "Implement MCP Configuration Integration",
      "description": "Integrate MCP server settings with the existing configuration system.",
      "details": "Extend the existing Settings class to include MCP configuration:\n1. Add MCP-specific settings to the Settings class\n2. Implement environment variable support\n3. Add configuration file integration\n4. Implement validation for MCP settings\n\nSettings should include:\n- Server transport method\n- Port and host for HTTP transport\n- Security settings\n- Tool enablement flags\n- Resource constraints\n\nExample implementation:\n```python\nclass MCPSettings(BaseSettings):\n    transport_type: str = \"stdio\"\n    http_host: str = \"127.0.0.1\"\n    http_port: int = 8000\n    max_request_size_mb: int = 10\n    enable_authentication: bool = False\n    auth_token: Optional[str] = None\n    tool_timeout_seconds: int = 30\n    \n    @validator(\"transport_type\")\n    def validate_transport(cls, v):\n        if v not in [\"stdio\", \"http\"]:\n            raise ValueError(\"Transport must be 'stdio' or 'http'\")\n        return v\n\n# Extend main settings\nclass Settings(BaseSettings):\n    # Existing settings...\n    mcp: MCPSettings = MCPSettings()\n```\n\nEnsure settings can be loaded from environment variables with prefix `PYTEST_ANALYZER_MCP_`.",
      "testStrategy": "Unit test settings initialization with default values. Test loading settings from environment variables. Verify validation logic for invalid settings. Test integration with existing settings system. Check configuration file loading.",
      "priority": "high",
      "dependencies": [
        3
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 6,
      "title": "Define Input/Output Schemas",
      "description": "Create Pydantic models for all MCP tool inputs and outputs to ensure proper validation and serialization.",
      "details": "In `src/pytest_analyzer/mcp/schemas/`, implement:\n1. Base request/response models\n2. Tool-specific input schemas\n3. Tool-specific output schemas\n4. Error response schemas\n5. Data transfer objects for complex objects\n\nImplement schemas for all tools mentioned in the PRD:\n- analyze_pytest_output\n- run_and_analyze\n- suggest_fixes\n- apply_suggestion\n- validate_suggestion\n- get_failure_summary\n- get_test_coverage\n- get_config\n- update_config\n\nExample implementation:\n```python\nfrom pydantic import BaseModel, Field, validator\nfrom typing import List, Optional, Dict, Any, Union\nimport os\n\nclass AnalyzePytestOutputRequest(BaseModel):\n    file_path: str = Field(..., description=\"Path to pytest output file\")\n    \n    @validator(\"file_path\")\n    def validate_file_exists(cls, v):\n        if not os.path.exists(v):\n            raise ValueError(f\"File does not exist: {v}\")\n        return v\n\nclass FixSuggestion(BaseModel):\n    file_path: str\n    line_number: int\n    original_code: str\n    suggested_code: str\n    confidence: float = Field(..., ge=0.0, le=1.0)\n    explanation: str\n\nclass AnalyzePytestOutputResponse(BaseModel):\n    suggestions: List[FixSuggestion]\n    execution_time_ms: int\n```\n\nEnsure all schemas have proper validation, documentation, and examples.",
      "testStrategy": "Unit test schema validation with valid and invalid inputs. Test serialization/deserialization of complex objects. Verify validation error messages are helpful. Test integration with MCP protocol handler. Check JSON schema generation for MCP compatibility.",
      "priority": "high",
      "dependencies": [
        1,
        2
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 7,
      "title": "Implement Facade Integration Pattern",
      "description": "Create a thin facade layer over the existing PytestAnalyzerFacade to maintain consistency with the current CLI interface.",
      "details": "Implement an MCP-specific facade that delegates to the existing PytestAnalyzerFacade:\n1. Create `src/pytest_analyzer/mcp/facade.py`\n2. Implement `MCPAnalyzerFacade` class that wraps the existing facade\n3. Add methods corresponding to MCP tools\n4. Ensure consistent error handling and logging\n5. Maintain dependency injection pattern\n\nExample implementation:\n```python\nfrom pytest_analyzer.core.facade import PytestAnalyzerFacade\nfrom pytest_analyzer.core.logging import get_logger\nfrom pytest_analyzer.mcp.schemas import *\n\nclass MCPAnalyzerFacade:\n    def __init__(self, analyzer_facade: PytestAnalyzerFacade):\n        self.analyzer_facade = analyzer_facade\n        self.logger = get_logger(__name__)\n    \n    async def analyze_pytest_output(self, request: AnalyzePytestOutputRequest) -> AnalyzePytestOutputResponse:\n        try:\n            self.logger.info(f\"Analyzing pytest output from {request.file_path}\")\n            result = self.analyzer_facade.analyze_file(request.file_path)\n            return AnalyzePytestOutputResponse(\n                suggestions=[\n                    FixSuggestion(**suggestion) for suggestion in result.suggestions\n                ],\n                execution_time_ms=result.execution_time_ms\n            )\n        except Exception as e:\n            self.logger.error(f\"Error analyzing pytest output: {str(e)}\")\n            raise\n```\n\nEnsure all facade methods are properly documented and follow existing patterns.",
      "testStrategy": "Unit test facade with mock analyzer facade. Verify correct delegation to underlying facade methods. Test error handling and propagation. Check logging integration. Verify input/output transformation between facades.",
      "priority": "high",
      "dependencies": [
        3,
        5,
        6
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 8,
      "title": "Implement Core Analysis Tool: analyze_pytest_output",
      "description": "Implement the MCP tool for analyzing pytest output files and generating fix suggestions.",
      "details": "In `src/pytest_analyzer/mcp/tools/analysis.py`, implement:\n1. `analyze_pytest_output` tool function\n2. Input validation using schemas\n3. Integration with MCPAnalyzerFacade\n4. Error handling and logging\n5. Performance monitoring\n\nImplementation should:\n- Accept file path to pytest output (JSON, XML, or text)\n- Detect file format automatically\n- Validate file existence and size\n- Return structured list of fix suggestions with confidence scores\n- Include execution time metrics\n\nExample implementation:\n```python\nfrom pytest_analyzer.mcp.facade import MCPAnalyzerFacade\nfrom pytest_analyzer.mcp.schemas import AnalyzePytestOutputRequest, AnalyzePytestOutputResponse\nimport time\n\nasync def analyze_pytest_output(request: AnalyzePytestOutputRequest, facade: MCPAnalyzerFacade) -> AnalyzePytestOutputResponse:\n    start_time = time.time()\n    \n    # Validate file size\n    file_size = os.path.getsize(request.file_path) / (1024 * 1024)  # Size in MB\n    if file_size > 10:  # 10MB limit\n        raise ValueError(f\"File too large: {file_size:.2f}MB (max 10MB)\")\n    \n    # Delegate to facade\n    result = await facade.analyze_pytest_output(request)\n    \n    # Add execution time\n    execution_time_ms = int((time.time() - start_time) * 1000)\n    result.execution_time_ms = execution_time_ms\n    \n    return result\n```\n\nRegister this tool with the MCP server during initialization.",
      "testStrategy": "Unit test with mock facade. Test with various pytest output formats (JSON, XML, text). Verify file validation logic works correctly. Test with large files to check size limits. Measure performance with realistic test data. Integration test with actual facade implementation.",
      "priority": "high",
      "dependencies": [
        6,
        7
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 9,
      "title": "Implement Core Analysis Tool: run_and_analyze",
      "description": "Implement the MCP tool for running pytest and analyzing the results in a single operation.",
      "details": "In `src/pytest_analyzer/mcp/tools/analysis.py`, implement:\n1. `run_and_analyze` tool function\n2. Input validation for test paths and arguments\n3. Sanitization of pytest arguments\n4. Integration with MCPAnalyzerFacade\n5. Timeout handling for long-running tests\n\nImplementation should:\n- Accept test path and optional pytest arguments\n- Support quiet mode to suppress console output\n- Validate path existence and argument safety\n- Execute pytest with specified arguments\n- Analyze results and return fix suggestions\n- Include execution metrics\n\nExample implementation:\n```python\nfrom pytest_analyzer.mcp.schemas import RunAndAnalyzeRequest, RunAndAnalyzeResponse\nimport shlex\nimport asyncio\n\nasync def run_and_analyze(request: RunAndAnalyzeRequest, facade: MCPAnalyzerFacade) -> RunAndAnalyzeResponse:\n    # Validate test path\n    if not os.path.exists(request.test_path):\n        raise ValueError(f\"Test path does not exist: {request.test_path}\")\n    \n    # Sanitize pytest arguments\n    safe_args = []\n    if request.pytest_args:\n        for arg in request.pytest_args:\n            # Validate argument safety\n            if arg.startswith(\"--\") or arg.startswith(\"-\"):\n                safe_args.append(arg)\n            else:\n                # For positional args, ensure they're paths that exist\n                if os.path.exists(arg):\n                    safe_args.append(arg)\n                else:\n                    raise ValueError(f\"Invalid pytest argument: {arg}\")\n    \n    # Run with timeout\n    try:\n        result = await asyncio.wait_for(\n            facade.run_and_analyze(request.test_path, safe_args, request.quiet),\n            timeout=request.timeout_seconds or 30\n        )\n        return RunAndAnalyzeResponse(\n            exit_code=result.exit_code,\n            suggestions=result.suggestions,\n            execution_time_ms=result.execution_time_ms,\n            output=result.output if not request.quiet else None\n        )\n    except asyncio.TimeoutError:\n        raise TimeoutError(f\"Test execution timed out after {request.timeout_seconds or 30} seconds\")\n```\n\nRegister this tool with the MCP server during initialization.",
      "testStrategy": "Unit test with mock facade. Test argument sanitization with valid and invalid inputs. Verify timeout handling works correctly. Test with various test paths and configurations. Integration test with actual pytest execution. Verify output capture in both quiet and verbose modes.",
      "priority": "high",
      "dependencies": [
        6,
        7,
        8
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 10,
      "title": "Implement Core Analysis Tool: suggest_fixes",
      "description": "Implement the MCP tool for generating fix suggestions from raw pytest output.",
      "details": "In `src/pytest_analyzer/mcp/tools/analysis.py`, implement:\n1. `suggest_fixes` tool function\n2. Input validation for pytest output string\n3. Size limit enforcement\n4. Integration with MCPAnalyzerFacade\n5. Format detection and parsing\n\nImplementation should:\n- Accept raw pytest output string\n- Detect output format (JSON, XML, text)\n- Validate input size limits\n- Parse and analyze the output\n- Return structured fix suggestions with code changes\n- Include confidence scores and explanations\n\nExample implementation:\n```python\nfrom pytest_analyzer.mcp.schemas import SuggestFixesRequest, SuggestFixesResponse\n\nasync def suggest_fixes(request: SuggestFixesRequest, facade: MCPAnalyzerFacade) -> SuggestFixesResponse:\n    # Validate input size\n    if len(request.pytest_output) > 1_000_000:  # 1MB limit\n        raise ValueError(f\"Input too large: {len(request.pytest_output)} bytes (max 1MB)\")\n    \n    # Detect format\n    output_format = \"text\"  # Default\n    if request.pytest_output.startswith(\"{\"):\n        output_format = \"json\"\n    elif request.pytest_output.startswith(\"<?xml\"):\n        output_format = \"xml\"\n    \n    # Delegate to facade\n    result = await facade.suggest_fixes_from_string(\n        request.pytest_output,\n        format=output_format\n    )\n    \n    return SuggestFixesResponse(\n        suggestions=result.suggestions,\n        execution_time_ms=result.execution_time_ms,\n        detected_format=output_format\n    )\n```\n\nRegister this tool with the MCP server during initialization.",
      "testStrategy": "Unit test with mock facade. Test format detection with various input types. Verify size limit enforcement. Test with realistic pytest outputs of different formats. Measure performance with large inputs. Integration test with actual facade implementation.",
      "priority": "high",
      "dependencies": [
        6,
        7
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 11,
      "title": "Implement Fix Application Tool: apply_suggestion",
      "description": "Implement the MCP tool for safely applying fix suggestions to code files.",
      "details": "In `src/pytest_analyzer/mcp/tools/fixes.py`, implement:\n1. `apply_suggestion` tool function\n2. Input validation for suggestion object\n3. File permission checking\n4. Backup creation before changes\n5. Git integration when available\n\nImplementation should:\n- Accept fix suggestion object and optional target files list\n- Validate file permissions and existence\n- Create backups before applying changes\n- Apply changes safely with proper error handling\n- Support Git integration for commit/branch creation\n- Return application result with success status and modified files\n\nExample implementation:\n```python\nfrom pytest_analyzer.mcp.schemas import ApplySuggestionRequest, ApplySuggestionResponse\nimport shutil\nimport subprocess\n\nasync def apply_suggestion(request: ApplySuggestionRequest, facade: MCPAnalyzerFacade) -> ApplySuggestionResponse:\n    modified_files = []\n    backup_files = []\n    \n    try:\n        # Validate target files\n        target_files = request.target_files or [request.suggestion.file_path]\n        for file_path in target_files:\n            if not os.path.exists(file_path):\n                raise ValueError(f\"Target file does not exist: {file_path}\")\n            if not os.access(file_path, os.W_OK):\n                raise ValueError(f\"No write permission for file: {file_path}\")\n            \n            # Create backup\n            backup_path = f\"{file_path}.bak\"\n            shutil.copy2(file_path, backup_path)\n            backup_files.append(backup_path)\n        \n        # Check for Git repository\n        git_available = False\n        if request.use_git:\n            try:\n                subprocess.run([\"git\", \"rev-parse\", \"--is-inside-work-tree\"], \n                              check=True, capture_output=True, text=True)\n                git_available = True\n            except (subprocess.SubprocessError, FileNotFoundError):\n                pass\n        \n        # Apply changes\n        result = await facade.apply_suggestion(\n            suggestion=request.suggestion,\n            target_files=target_files,\n            create_backup=False  # We already created backups\n        )\n        \n        modified_files = result.modified_files\n        \n        # Git commit if requested and available\n        git_info = None\n        if git_available and request.use_git:\n            branch_name = f\"fix-{int(time.time())}\"\n            subprocess.run([\"git\", \"checkout\", \"-b\", branch_name], check=True)\n            subprocess.run([\"git\", \"add\"] + modified_files, check=True)\n            commit_msg = f\"Apply fix suggestion: {request.suggestion.explanation}\"\n            subprocess.run([\"git\", \"commit\", \"-m\", commit_msg], check=True)\n            git_info = {\"branch\": branch_name, \"commit_message\": commit_msg}\n        \n        return ApplySuggestionResponse(\n            success=True,\n            modified_files=modified_files,\n            backup_files=backup_files,\n            git_info=git_info\n        )\n    except Exception as e:\n        # Rollback on error\n        for i, backup_path in enumerate(backup_files):\n            original_path = target_files[i]\n            if os.path.exists(backup_path):\n                shutil.copy2(backup_path, original_path)\n        \n        raise\n```\n\nRegister this tool with the MCP server during initialization.",
      "testStrategy": "Unit test with mock facade. Test file permission checking. Verify backup creation and restoration on error. Test Git integration with mock subprocess. Integration test with actual file modifications. Verify rollback functionality works correctly.",
      "priority": "medium",
      "dependencies": [
        6,
        7,
        10
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 12,
      "title": "Implement Fix Application Tool: validate_suggestion",
      "description": "Implement the MCP tool for validating fix suggestions without applying changes.",
      "details": "In `src/pytest_analyzer/mcp/tools/fixes.py`, implement:\n1. `validate_suggestion` tool function\n2. Input validation for suggestion object\n3. Syntax checking for suggested code\n4. Conflict detection with existing code\n5. Validation without applying changes\n\nImplementation should:\n- Accept fix suggestion object and target files\n- Validate file existence and readability\n- Check syntax of suggested code changes\n- Detect potential conflicts or issues\n- Return validation result without modifying files\n\nExample implementation:\n```python\nfrom pytest_analyzer.mcp.schemas import ValidateSuggestionRequest, ValidateSuggestionResponse\nimport ast\n\nasync def validate_suggestion(request: ValidateSuggestionRequest, facade: MCPAnalyzerFacade) -> ValidateSuggestionResponse:\n    validation_results = []\n    \n    # Validate target files\n    target_files = request.target_files or [request.suggestion.file_path]\n    for file_path in target_files:\n        if not os.path.exists(file_path):\n            raise ValueError(f\"Target file does not exist: {file_path}\")\n        if not os.access(file_path, os.R_OK):\n            raise ValueError(f\"No read permission for file: {file_path}\")\n    \n    # Validate syntax of suggested code\n    syntax_valid = True\n    syntax_error = None\n    try:\n        ast.parse(request.suggestion.suggested_code)\n    except SyntaxError as e:\n        syntax_valid = False\n        syntax_error = str(e)\n    \n    # Delegate to facade for deeper validation\n    result = await facade.validate_suggestion(\n        suggestion=request.suggestion,\n        target_files=target_files\n    )\n    \n    return ValidateSuggestionResponse(\n        valid=result.valid and syntax_valid,\n        syntax_valid=syntax_valid,\n        syntax_error=syntax_error,\n        conflicts=result.conflicts,\n        warnings=result.warnings\n    )\n```\n\nRegister this tool with the MCP server during initialization.",
      "testStrategy": "Unit test with mock facade. Test syntax validation with valid and invalid code. Verify file existence checking. Test conflict detection with various code scenarios. Integration test with actual code files. Verify no modifications are made to files during validation.",
      "priority": "medium",
      "dependencies": [
        6,
        7,
        11
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 13,
      "title": "Implement Information Tool: get_failure_summary",
      "description": "Implement the MCP tool for generating statistical summaries of test failures.",
      "details": "In `src/pytest_analyzer/mcp/tools/information.py`, implement:\n1. `get_failure_summary` tool function\n2. Input validation for test results or report path\n3. Failure categorization and analysis\n4. Statistical summary generation\n5. Trend analysis when historical data available\n\nImplementation should:\n- Accept test results or report path\n- Validate data format\n- Categorize failures by type and location\n- Generate statistical summaries with counts and percentages\n- Include trends if historical data is available\n\nExample implementation:\n```python\nfrom pytest_analyzer.mcp.schemas import GetFailureSummaryRequest, GetFailureSummaryResponse\n\nasync def get_failure_summary(request: GetFailureSummaryRequest, facade: MCPAnalyzerFacade) -> GetFailureSummaryResponse:\n    # Validate input\n    if request.report_path and not os.path.exists(request.report_path):\n        raise ValueError(f\"Report file does not exist: {request.report_path}\")\n    \n    # Delegate to facade\n    if request.report_path:\n        result = await facade.get_failure_summary_from_file(request.report_path)\n    else:\n        result = await facade.get_failure_summary_from_results(request.test_results)\n    \n    # Build response\n    return GetFailureSummaryResponse(\n        total_tests=result.total_tests,\n        failed_tests=result.failed_tests,\n        passed_tests=result.passed_tests,\n        error_tests=result.error_tests,\n        skipped_tests=result.skipped_tests,\n        failure_categories=result.failure_categories,\n        most_common_failures=result.most_common_failures,\n        trends=result.trends if request.include_trends else None,\n        execution_time_ms=result.execution_time_ms\n    )\n```\n\nRegister this tool with the MCP server during initialization.",
      "testStrategy": "Unit test with mock facade. Test with various report formats. Verify statistical calculations are correct. Test trend analysis with historical data. Integration test with actual test results. Verify categorization logic works correctly.",
      "priority": "medium",
      "dependencies": [
        6,
        7
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 14,
      "title": "Implement Information Tool: get_test_coverage",
      "description": "Implement the MCP tool for retrieving test coverage information when available.",
      "details": "In `src/pytest_analyzer/mcp/tools/information.py`, implement:\n1. `get_test_coverage` tool function\n2. Input validation for test execution results\n3. Coverage data availability checking\n4. Integration with pytest-cov when available\n5. Coverage summary generation\n\nImplementation should:\n- Accept test execution results\n- Check if coverage data is available\n- Extract coverage information from pytest-cov data\n- Generate coverage summary with file and line statistics\n- Handle cases where coverage data is not available\n\nExample implementation:\n```python\nfrom pytest_analyzer.mcp.schemas import GetTestCoverageRequest, GetTestCoverageResponse\n\nasync def get_test_coverage(request: GetTestCoverageRequest, facade: MCPAnalyzerFacade) -> GetTestCoverageResponse:\n    # Check if coverage data is available\n    has_coverage = await facade.has_coverage_data(request.test_results)\n    \n    if not has_coverage:\n        return GetTestCoverageResponse(\n            available=False,\n            message=\"No coverage data available. Run tests with pytest-cov plugin.\"\n        )\n    \n    # Delegate to facade\n    result = await facade.get_test_coverage(request.test_results)\n    \n    # Build response\n    return GetTestCoverageResponse(\n        available=True,\n        total_coverage_percent=result.total_coverage_percent,\n        file_coverage=result.file_coverage,\n        uncovered_lines=result.uncovered_lines,\n        coverage_report=result.coverage_report\n    )\n```\n\nRegister this tool with the MCP server during initialization.",
      "testStrategy": "Unit test with mock facade. Test with and without coverage data available. Verify correct handling when coverage is not available. Test with various coverage percentages and file statistics. Integration test with actual pytest-cov data.",
      "priority": "medium",
      "dependencies": [
        6,
        7
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 15,
      "title": "Implement Configuration Tool: get_config",
      "description": "Implement the MCP tool for retrieving current analyzer configuration.",
      "details": "In `src/pytest_analyzer/mcp/tools/configuration.py`, implement:\n1. `get_config` tool function\n2. Configuration access permission checking\n3. Sensitive data filtering\n4. Configuration retrieval from Settings\n\nImplementation should:\n- Retrieve current analyzer configuration\n- Filter out sensitive information\n- Format configuration for client consumption\n- Include descriptions and allowed values\n\nExample implementation:\n```python\nfrom pytest_analyzer.mcp.schemas import GetConfigRequest, GetConfigResponse\nfrom pytest_analyzer.core.settings import get_settings\n\nasync def get_config(request: GetConfigRequest, facade: MCPAnalyzerFacade) -> GetConfigResponse:\n    # Get current settings\n    settings = get_settings()\n    \n    # Filter sensitive information\n    config_dict = settings.dict(exclude={\"auth_token\", \"api_keys\", \"credentials\"})\n    \n    # Add descriptions and metadata\n    config_with_metadata = {}\n    for key, value in config_dict.items():\n        if key == \"mcp\":\n            # Handle MCP settings specially\n            mcp_config = {}\n            for mcp_key, mcp_value in value.items():\n                mcp_config[mcp_key] = {\n                    \"value\": mcp_value,\n                    \"description\": get_setting_description(f\"mcp.{mcp_key}\"),\n                    \"allowed_values\": get_allowed_values(f\"mcp.{mcp_key}\")\n                }\n            config_with_metadata[key] = mcp_config\n        else:\n            config_with_metadata[key] = {\n                \"value\": value,\n                \"description\": get_setting_description(key),\n                \"allowed_values\": get_allowed_values(key)\n            }\n    \n    return GetConfigResponse(\n        config=config_with_metadata,\n        sections=[\"general\", \"analysis\", \"mcp\", \"logging\"]\n    )\n\ndef get_setting_description(key):\n    # Implementation to get setting descriptions\n    descriptions = {\n        \"mcp.transport_type\": \"Transport method for MCP server (stdio or http)\",\n        \"mcp.http_port\": \"Port for HTTP transport\",\n        # More descriptions...\n    }\n    return descriptions.get(key, \"\")\n\ndef get_allowed_values(key):\n    # Implementation to get allowed values\n    allowed_values = {\n        \"mcp.transport_type\": [\"stdio\", \"http\"],\n        # More allowed values...\n    }\n    return allowed_values.get(key, None)\n```\n\nRegister this tool with the MCP server during initialization.",
      "testStrategy": "Unit test with mock settings. Verify sensitive information is properly filtered. Test with various configuration values. Check that descriptions and allowed values are correctly included. Integration test with actual settings system.",
      "priority": "low",
      "dependencies": [
        5,
        6,
        7
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 16,
      "title": "Implement Configuration Tool: update_config",
      "description": "Implement the MCP tool for safely updating analyzer configuration.",
      "details": "In `src/pytest_analyzer/mcp/tools/configuration.py`, implement:\n1. `update_config` tool function\n2. Input validation for configuration updates\n3. Permission checking for configuration changes\n4. Safe configuration updating with validation\n\nImplementation should:\n- Accept configuration updates\n- Validate configuration value types and constraints\n- Apply changes safely with proper error handling\n- Persist configuration changes when appropriate\n- Return updated configuration confirmation\n\nExample implementation:\n```python\nfrom pytest_analyzer.mcp.schemas import UpdateConfigRequest, UpdateConfigResponse\nfrom pytest_analyzer.core.settings import get_settings, save_settings\n\nasync def update_config(request: UpdateConfigRequest, facade: MCPAnalyzerFacade) -> UpdateConfigResponse:\n    # Get current settings\n    settings = get_settings()\n    \n    # Track changes and validation errors\n    changes = {}\n    validation_errors = {}\n    \n    # Process updates\n    for key, value in request.updates.items():\n        # Handle nested MCP settings\n        if key.startswith(\"mcp.\"):\n            _, mcp_key = key.split(\".\", 1)\n            try:\n                # Validate value\n                validate_setting_value(\"mcp.\" + mcp_key, value)\n                \n                # Apply change\n                setattr(settings.mcp, mcp_key, value)\n                changes[key] = value\n            except ValueError as e:\n                validation_errors[key] = str(e)\n        else:\n            try:\n                # Validate value\n                validate_setting_value(key, value)\n                \n                # Apply change\n                setattr(settings, key, value)\n                changes[key] = value\n            except ValueError as e:\n                validation_errors[key] = str(e)\n    \n    # Save changes if requested and no errors\n    if not validation_errors and request.persist:\n        save_settings(settings)\n    \n    return UpdateConfigResponse(\n        success=len(validation_errors) == 0,\n        changes=changes,\n        validation_errors=validation_errors,\n        persisted=request.persist and len(validation_errors) == 0\n    )\n\ndef validate_setting_value(key, value):\n    # Implementation to validate setting values\n    validators = {\n        \"mcp.transport_type\": lambda v: v in [\"stdio\", \"http\"],\n        \"mcp.http_port\": lambda v: isinstance(v, int) and 1024 <= v <= 65535,\n        # More validators...\n    }\n    \n    if key in validators and not validators[key](value):\n        allowed = get_allowed_values(key)\n        allowed_str = f\" (allowed: {allowed})\" if allowed else \"\"\n        raise ValueError(f\"Invalid value for {key}{allowed_str}\")\n```\n\nRegister this tool with the MCP server during initialization.",
      "testStrategy": "Unit test with mock settings. Test validation with valid and invalid values. Verify changes are correctly applied to settings. Test persistence with mock save function. Integration test with actual settings system. Verify error handling for invalid updates.",
      "priority": "low",
      "dependencies": [
        5,
        6,
        7,
        15
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 17,
      "title": "Implement MCP Resource Definitions",
      "description": "Define MCP resources for accessing test data and analysis results.",
      "details": "In `src/pytest_analyzer/mcp/resources.py`, implement:\n1. Resource URI patterns and handlers\n2. Session management for resources\n3. Data serialization for resource access\n4. Access control and validation\n\nImplement the following resources:\n1. **Test Results** (`test-results://session/{id}`)\n   - Access to parsed test execution results\n   - Filterable by status, type, or file\n\n2. **Fix Suggestions** (`suggestions://session/{id}`)\n   - Access to generated fix suggestions\n   - Organized by failure ID or confidence level\n\n3. **Analysis History** (`history://recent`)\n   - Access to recent analysis sessions\n   - Time-based filtering and pagination\n\nExample implementation:\n```python\nfrom mcp_python_sdk import ResourceHandler, ResourceRequest, ResourceResponse\nfrom pytest_analyzer.core.session import SessionManager\n\nclass TestResultsResource(ResourceHandler):\n    def __init__(self, session_manager: SessionManager):\n        self.session_manager = session_manager\n    \n    async def handle_request(self, request: ResourceRequest) -> ResourceResponse:\n        # Parse URI\n        parts = request.uri.split(\"/\")\n        if len(parts) < 3 or parts[0] != \"test-results:\" or parts[1] != \"session\":\n            return ResourceResponse(status=400, body={\"error\": \"Invalid resource URI\"})\n        \n        session_id = parts[2]\n        \n        # Get session data\n        session = self.session_manager.get_session(session_id)\n        if not session:\n            return ResourceResponse(status=404, body={\"error\": \"Session not found\"})\n        \n        # Apply filters\n        filters = request.query_params or {}\n        results = session.get_test_results(filters)\n        \n        # Paginate if needed\n        page = int(filters.get(\"page\", 1))\n        page_size = int(filters.get(\"page_size\", 50))\n        start = (page - 1) * page_size\n        end = start + page_size\n        paginated_results = results[start:end]\n        \n        return ResourceResponse(\n            status=200,\n            body={\n                \"results\": paginated_results,\n                \"total\": len(results),\n                \"page\": page,\n                \"page_size\": page_size,\n                \"pages\": (len(results) + page_size - 1) // page_size\n            }\n        )\n```\n\nRegister these resources with the MCP server during initialization.",
      "testStrategy": "Unit test resource handlers with mock session data. Test URI parsing and validation. Verify filtering works correctly. Test pagination with various page sizes. Integration test with actual session data. Verify error handling for invalid requests.",
      "priority": "medium",
      "dependencies": [
        3,
        6
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 18,
      "title": "Implement Logging and Monitoring Integration",
      "description": "Integrate MCP server operations with existing logging and monitoring systems.",
      "details": "Implement comprehensive logging and monitoring for the MCP server:\n1. Extend existing logging system for MCP events\n2. Add structured logging for protocol messages\n3. Implement performance metrics collection\n4. Add security event logging\n\nImplementation should:\n- Log MCP protocol messages at debug level\n- Log tool execution with performance metrics\n- Log errors with detailed context\n- Log security events (authentication, authorization)\n- Support existing logging configuration\n\nExample implementation:\n```python\nfrom pytest_analyzer.core.logging import get_logger\nimport time\nimport json\n\nclass MCPLogger:\n    def __init__(self):\n        self.logger = get_logger(\"pytest_analyzer.mcp\")\n        self.protocol_logger = get_logger(\"pytest_analyzer.mcp.protocol\")\n        self.security_logger = get_logger(\"pytest_analyzer.mcp.security\")\n        self.performance_logger = get_logger(\"pytest_analyzer.mcp.performance\")\n    \n    def log_protocol_message(self, direction, message):\n        \"\"\"Log MCP protocol message at debug level.\"\"\"\n        self.protocol_logger.debug(\n            f\"{direction} MCP message\",\n            extra={\"message\": self._sanitize_message(message)}\n        )\n    \n    def log_tool_execution(self, tool_name, start_time, end_time, success, error=None):\n        \"\"\"Log tool execution with performance metrics.\"\"\"\n        duration_ms = int((end_time - start_time) * 1000)\n        self.performance_logger.info(\n            f\"Tool execution: {tool_name}\",\n            extra={\n                \"tool\": tool_name,\n                \"duration_ms\": duration_ms,\n                \"success\": success,\n                \"error\": str(error) if error else None\n            }\n        )\n    \n    def log_security_event(self, event_type, details):\n        \"\"\"Log security-related events.\"\"\"\n        self.security_logger.info(\n            f\"Security event: {event_type}\",\n            extra={\"event_type\": event_type, \"details\": details}\n        )\n    \n    def _sanitize_message(self, message):\n        \"\"\"Sanitize sensitive information from messages.\"\"\"\n        if isinstance(message, dict):\n            sanitized = message.copy()\n            if \"auth_token\" in sanitized:\n                sanitized[\"auth_token\"] = \"[REDACTED]\"\n            # More sanitization rules...\n            return sanitized\n        return message\n```\n\nIntegrate this logger with the MCP server and tool implementations.",
      "testStrategy": "Unit test logger with mock messages. Verify sanitization works correctly for sensitive data. Test performance logging with timing information. Check integration with existing logging configuration. Verify structured logging format is correct.",
      "priority": "medium",
      "dependencies": [
        3,
        5
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 19,
      "title": "Implement Security Controls",
      "description": "Implement comprehensive security controls for the MCP server.",
      "details": "Implement security controls for the MCP server:\n1. Input validation for all tool inputs\n2. File system access controls\n3. Optional authentication and authorization\n4. Rate limiting and abuse prevention\n\nImplementation should include:\n\n**Input Validation**:\n- Path traversal prevention\n- File size and type restrictions\n- Command injection prevention\n- Input sanitization and escaping\n\n**File System Access**:\n- Restricted to project directory by default\n- Configurable path allowlists\n- Read-only vs. read-write permissions\n- Backup and rollback for write operations\n\n**Authentication** (for HTTP transport):\n- Token-based authentication\n- Client certificate validation\n- Role-based access control\n\n**Rate Limiting**:\n- Request rate limiting\n- Resource usage monitoring\n- Abuse detection and prevention\n\nExample implementation for path validation:\n```python\nimport os\nimport pathlib\n\nclass SecurityManager:\n    def __init__(self, settings):\n        self.settings = settings\n        self.allowed_paths = self._get_allowed_paths()\n    \n    def _get_allowed_paths(self):\n        \"\"\"Get list of allowed paths from settings.\"\"\"\n        paths = [os.getcwd()]  # Current directory always allowed\n        if hasattr(self.settings, \"allowed_paths\") and self.settings.allowed_paths:\n            paths.extend(self.settings.allowed_paths)\n        return [os.path.abspath(p) for p in paths]\n    \n    def validate_path(self, path, require_write=False):\n        \"\"\"Validate if a path is allowed for access.\"\"\"\n        abs_path = os.path.abspath(path)\n        \n        # Check if path exists\n        if not os.path.exists(abs_path):\n            raise ValueError(f\"Path does not exist: {path}\")\n        \n        # Check if path is within allowed paths\n        if not any(self._is_subpath(abs_path, allowed) for allowed in self.allowed_paths):\n            raise ValueError(f\"Access to path not allowed: {path}\")\n        \n        # Check write permission if required\n        if require_write and not os.access(abs_path, os.W_OK):\n            raise ValueError(f\"No write permission for path: {path}\")\n        \n        return abs_path\n    \n    def _is_subpath(self, path, parent):\n        \"\"\"Check if path is a subpath of parent.\"\"\"\n        path_obj = pathlib.Path(path).resolve()\n        parent_obj = pathlib.Path(parent).resolve()\n        \n        try:\n            path_obj.relative_to(parent_obj)\n            return True\n        except ValueError:\n            return False\n```\n\nIntegrate the SecurityManager with all MCP tools that access files or execute commands.",
      "testStrategy": "Unit test path validation with various paths. Test with paths outside allowed directories. Verify write permission checking. Test authentication with valid and invalid tokens. Test rate limiting with rapid requests. Security audit with automated scanning tools.",
      "priority": "high",
      "dependencies": [
        3,
        5,
        18
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 20,
      "title": "Create Comprehensive Documentation",
      "description": "Create complete documentation for the MCP server features and integration.",
      "details": "Create comprehensive documentation for the MCP server:\n1. API documentation for all MCP tools\n2. Integration guide for Claude Desktop\n3. Configuration reference\n4. Troubleshooting and FAQ\n\nDocumentation should include:\n\n**API Documentation**:\n- Auto-generated from schemas\n- Complete description of all tools\n- Input and output formats\n- Error codes and handling\n- Examples for each tool\n\n**Integration Guide**:\n- Step-by-step setup with Claude Desktop\n- Configuration options\n- Example workflows\n- Best practices\n\n**Configuration Reference**:\n- All configuration options\n- Environment variables\n- Configuration file format\n- Default values and constraints\n\n**Troubleshooting**:\n- Common issues and solutions\n- Logging and debugging\n- Error message reference\n- Support resources\n\nImplementation approach:\n1. Use auto-documentation from Pydantic schemas\n2. Create Markdown documentation in `docs/mcp/`\n3. Include code examples for all tools\n4. Create integration tutorials with screenshots\n5. Add troubleshooting guide with common issues\n\nExample documentation structure:\n```\ndocs/mcp/\n├── index.md                 # Overview and introduction\n├── installation.md          # Installation and setup\n├── configuration.md         # Configuration reference\n├── tools/                   # Tool documentation\n│   ├── analysis.md          # Analysis tools\n│   ├── fixes.md             # Fix application tools\n│   ├── information.md       # Information tools\n│   └── configuration.md     # Configuration tools\n├── resources/               # Resource documentation\n├── integration/             # Integration guides\n│   ├── claude-desktop.md    # Claude Desktop integration\n│   └── custom-clients.md    # Custom client integration\n├── security.md              # Security considerations\n├── troubleshooting.md       # Troubleshooting guide\n└── faq.md                   # Frequently asked questions\n```",
      "testStrategy": "Review documentation for completeness and accuracy. Verify code examples work as documented. Test integration guide with actual Claude Desktop. Have team members follow documentation to verify clarity. Check for broken links and outdated information.",
      "priority": "high",
      "dependencies": [
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9,
        10,
        11,
        12,
        13,
        14,
        15,
        16,
        17,
        18,
        19
      ],
      "status": "pending",
      "subtasks": []
    }
  ]
}
