{
  "tasks": [
    {
      "id": 1,
      "title": "Create Dependency Injection Framework",
      "description": "Implement a dependency injection framework to decouple components and improve testability",
      "details": "Create a DI container class that will manage component registration and resolution. Implement methods for registering components, resolving dependencies, and managing lifecycle. Use Python's typing system to ensure type safety.\n\nExample implementation:\n```python\nfrom typing import Any, Dict, Type, TypeVar, cast\n\nT = TypeVar('T')\n\nclass DIContainer:\n    def __init__(self):\n        self._services: Dict[Type, Any] = {}\n        \n    def register(self, interface_type: Type[T], implementation: T) -> None:\n        self._services[interface_type] = implementation\n        \n    def resolve(self, interface_type: Type[T]) -> T:\n        if interface_type not in self._services:\n            raise KeyError(f\"No implementation registered for {interface_type.__name__}\")\n        return cast(T, self._services[interface_type])\n```\n\nEnsure the container supports singleton and transient lifetimes for services.",
      "testStrategy": "Create unit tests for the DI container, testing registration, resolution, and error cases. Verify that circular dependencies are detected and proper error messages are shown. Test both singleton and transient service lifetimes.",
      "priority": "high",
      "dependencies": [],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 2,
      "title": "Define Protocol Interfaces",
      "description": "Create clear interfaces using Python's Protocol type for all major components",
      "details": "Define Protocol classes for all major components in the system: Extractor, Analyzer, Suggester, Applier, and LLM Service. Use Python's typing.Protocol to define these interfaces.\n\nExample implementation:\n```python\nfrom typing import Protocol, List, Optional\nfrom dataclasses import dataclass\n\n@dataclass\nclass TestFailure:\n    test_name: str\n    error_message: str\n    traceback: str\n\nclass Extractor(Protocol):\n    def extract_failures(self, test_output: str) -> List[TestFailure]:\n        ...\n        \nclass Analyzer(Protocol):\n    def analyze_failure(self, failure: TestFailure) -> str:\n        ...\n        \nclass Suggester(Protocol):\n    def suggest_fix(self, analysis: str, failure: TestFailure) -> str:\n        ...\n        \nclass Applier(Protocol):\n    def apply_fix(self, suggestion: str, file_path: str) -> bool:\n        ...\n        \nclass LLMService(Protocol):\n    def generate_completion(self, prompt: str) -> str:\n        ...\n    \n    async def generate_completion_async(self, prompt: str) -> str:\n        ...\n```\n\nEnsure all interfaces have proper docstrings explaining their purpose and contract.",
      "testStrategy": "Create test cases that verify implementations conform to the protocols. Use mypy or similar static type checkers to verify protocol conformance. Write documentation tests that demonstrate proper usage of each interface.",
      "priority": "high",
      "dependencies": [
        1
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 3,
      "title": "Implement Extractor Component",
      "description": "Refactor the test failure extraction logic into a dedicated component with proper interfaces",
      "details": "Create concrete implementations of the Extractor protocol. The extractor should parse pytest output and extract test failures. Implement proper error handling and logging.\n\nExample implementation:\n```python\nfrom typing import List, Optional\nimport re\nimport logging\nfrom .interfaces import Extractor, TestFailure\n\nclass PytestExtractor(Extractor):\n    def __init__(self, logger: Optional[logging.Logger] = None):\n        self.logger = logger or logging.getLogger(__name__)\n        \n    def extract_failures(self, test_output: str) -> List[TestFailure]:\n        self.logger.debug(f\"Extracting failures from {len(test_output)} bytes of test output\")\n        failures = []\n        \n        # Regex pattern to match pytest failures\n        failure_pattern = r\"E\\s+(.+?)\\n([\\s\\S]+?)(?=\\n\\n|$)\"\n        \n        try:\n            for match in re.finditer(failure_pattern, test_output):\n                error_message = match.group(1)\n                traceback = match.group(2)\n                \n                # Extract test name from traceback\n                test_name_match = re.search(r\"([\\w\\.]+)::([\\w\\.]+)\", traceback)\n                test_name = test_name_match.group(0) if test_name_match else \"Unknown test\"\n                \n                failures.append(TestFailure(\n                    test_name=test_name,\n                    error_message=error_message,\n                    traceback=traceback\n                ))\n                \n            self.logger.info(f\"Extracted {len(failures)} test failures\")\n            return failures\n        except Exception as e:\n            self.logger.error(f\"Error extracting failures: {str(e)}\")\n            raise\n```\n\nEnsure the implementation handles different pytest output formats and edge cases.",
      "testStrategy": "Create unit tests with sample pytest outputs of varying formats. Test error handling by providing malformed inputs. Verify that all extracted failures contain the correct information. Test with both successful and failing test outputs.",
      "priority": "medium",
      "dependencies": [
        2
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 4,
      "title": "Implement Analyzer Component",
      "description": "Refactor the test failure analysis logic into a dedicated component with proper interfaces",
      "details": "Create concrete implementations of the Analyzer protocol. The analyzer should analyze test failures and provide context for suggesting fixes. Implement proper error handling and logging.\n\nExample implementation:\n```python\nimport logging\nfrom typing import Optional\nfrom .interfaces import Analyzer, TestFailure, LLMService\n\nclass LLMBasedAnalyzer(Analyzer):\n    def __init__(self, llm_service: LLMService, logger: Optional[logging.Logger] = None):\n        self.llm_service = llm_service\n        self.logger = logger or logging.getLogger(__name__)\n        \n    def analyze_failure(self, failure: TestFailure) -> str:\n        self.logger.info(f\"Analyzing failure for test: {failure.test_name}\")\n        \n        try:\n            prompt = f\"\"\"Analyze the following pytest test failure and explain what might be causing it:\n            \nTest name: {failure.test_name}\nError message: {failure.error_message}\nTraceback:\n{failure.traceback}\n\nProvide a detailed analysis of what's causing this failure:\"\"\"\n            \n            analysis = self.llm_service.generate_completion(prompt)\n            self.logger.debug(f\"Generated analysis: {analysis[:100]}...\")\n            return analysis\n        except Exception as e:\n            self.logger.error(f\"Error analyzing test failure: {str(e)}\")\n            raise\n```\n\nEnsure the implementation handles different types of test failures and provides useful analysis.",
      "testStrategy": "Create unit tests with mock LLMService to verify the analyzer correctly formats prompts and handles responses. Test error handling by simulating LLM service failures. Verify that the analyzer correctly logs operations and errors.",
      "priority": "medium",
      "dependencies": [
        2,
        3
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 5,
      "title": "Implement Suggester Component",
      "description": "Refactor the fix suggestion logic into a dedicated component with proper interfaces",
      "details": "Create concrete implementations of the Suggester protocol. The suggester should generate fix suggestions based on the analysis. Implement proper error handling and logging.\n\nExample implementation:\n```python\nimport logging\nfrom typing import Optional\nfrom .interfaces import Suggester, TestFailure, LLMService\n\nclass LLMBasedSuggester(Suggester):\n    def __init__(self, llm_service: LLMService, logger: Optional[logging.Logger] = None):\n        self.llm_service = llm_service\n        self.logger = logger or logging.getLogger(__name__)\n        \n    def suggest_fix(self, analysis: str, failure: TestFailure) -> str:\n        self.logger.info(f\"Suggesting fix for test: {failure.test_name}\")\n        \n        try:\n            prompt = f\"\"\"Based on the following analysis of a pytest test failure, suggest a specific code fix:\n            \nTest name: {failure.test_name}\nError message: {failure.error_message}\nTraceback:\n{failure.traceback}\n\nAnalysis:\n{analysis}\n\nProvide a specific code fix that would resolve this issue:\"\"\"\n            \n            suggestion = self.llm_service.generate_completion(prompt)\n            self.logger.debug(f\"Generated suggestion: {suggestion[:100]}...\")\n            return suggestion\n        except Exception as e:\n            self.logger.error(f\"Error suggesting fix: {str(e)}\")\n            raise\n```\n\nEnsure the implementation generates useful and specific fix suggestions.",
      "testStrategy": "Create unit tests with mock LLMService to verify the suggester correctly formats prompts and handles responses. Test error handling by simulating LLM service failures. Verify that the suggester correctly logs operations and errors.",
      "priority": "medium",
      "dependencies": [
        2,
        4
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 6,
      "title": "Implement Applier Component",
      "description": "Refactor the fix application logic into a dedicated component with proper interfaces",
      "details": "Create concrete implementations of the Applier protocol. The applier should apply suggested fixes to the code. Implement proper error handling, logging, and file backup mechanisms.\n\nExample implementation:\n```python\nimport logging\nimport os\nimport re\nfrom typing import Optional\nfrom .interfaces import Applier\n\nclass FileSystemApplier(Applier):\n    def __init__(self, logger: Optional[logging.Logger] = None):\n        self.logger = logger or logging.getLogger(__name__)\n        \n    def apply_fix(self, suggestion: str, file_path: str) -> bool:\n        self.logger.info(f\"Applying fix to file: {file_path}\")\n        \n        if not os.path.exists(file_path):\n            self.logger.error(f\"File not found: {file_path}\")\n            return False\n            \n        try:\n            # Create backup of the file\n            backup_path = f\"{file_path}.bak\"\n            with open(file_path, 'r') as src, open(backup_path, 'w') as dst:\n                dst.write(src.read())\n                \n            # Extract code changes from suggestion\n            code_pattern = r\"```python\\n([\\s\\S]+?)\\n```\"\n            code_match = re.search(code_pattern, suggestion)\n            \n            if not code_match:\n                self.logger.warning(\"No code block found in suggestion\")\n                return False\n                \n            new_code = code_match.group(1)\n            \n            # Apply changes (simplified example - in reality would need to be more sophisticated)\n            with open(file_path, 'w') as f:\n                f.write(new_code)\n                \n            self.logger.info(f\"Successfully applied fix to {file_path}\")\n            return True\n        except Exception as e:\n            self.logger.error(f\"Error applying fix: {str(e)}\")\n            # Restore from backup if it exists\n            if os.path.exists(backup_path):\n                os.replace(backup_path, file_path)\n            return False\n```\n\nEnsure the implementation safely applies changes and includes rollback mechanisms.",
      "testStrategy": "Create unit tests with temporary files to verify the applier correctly applies changes. Test error handling by simulating file system errors. Verify that backups are created and restored properly in case of failures. Test with various suggestion formats.",
      "priority": "medium",
      "dependencies": [
        2,
        5
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 7,
      "title": "Implement LLM Service",
      "description": "Create a proper LLM service with configuration and resource management injected",
      "details": "Implement a service for interacting with language models. Support both synchronous and asynchronous APIs. Implement proper error handling, retries, and resource management.\n\nExample implementation:\n```python\nimport logging\nimport asyncio\nimport time\nfrom typing import Optional, Dict, Any\nimport aiohttp\nimport requests\nfrom .interfaces import LLMService\n\nclass OpenAIService(LLMService):\n    def __init__(self, api_key: str, model: str = \"gpt-4\", \n                 max_retries: int = 3, timeout: int = 30,\n                 logger: Optional[logging.Logger] = None):\n        self.api_key = api_key\n        self.model = model\n        self.max_retries = max_retries\n        self.timeout = timeout\n        self.logger = logger or logging.getLogger(__name__)\n        self.base_url = \"https://api.openai.com/v1/chat/completions\"\n        \n    def _prepare_payload(self, prompt: str) -> Dict[str, Any]:\n        return {\n            \"model\": self.model,\n            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n            \"temperature\": 0.7\n        }\n        \n    def generate_completion(self, prompt: str) -> str:\n        self.logger.debug(f\"Generating completion for prompt: {prompt[:50]}...\")\n        \n        headers = {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"Content-Type\": \"application/json\"\n        }\n        \n        payload = self._prepare_payload(prompt)\n        \n        for attempt in range(self.max_retries):\n            try:\n                response = requests.post(\n                    self.base_url,\n                    headers=headers,\n                    json=payload,\n                    timeout=self.timeout\n                )\n                \n                response.raise_for_status()\n                result = response.json()\n                completion = result[\"choices\"][0][\"message\"][\"content\"]\n                \n                self.logger.debug(f\"Generated completion: {completion[:50]}...\")\n                return completion\n            except Exception as e:\n                self.logger.warning(f\"Attempt {attempt+1} failed: {str(e)}\")\n                if attempt < self.max_retries - 1:\n                    time.sleep(2 ** attempt)  # Exponential backoff\n                else:\n                    self.logger.error(f\"All {self.max_retries} attempts failed\")\n                    raise\n    \n    async def generate_completion_async(self, prompt: str) -> str:\n        self.logger.debug(f\"Generating async completion for prompt: {prompt[:50]}...\")\n        \n        headers = {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"Content-Type\": \"application/json\"\n        }\n        \n        payload = self._prepare_payload(prompt)\n        \n        for attempt in range(self.max_retries):\n            try:\n                async with aiohttp.ClientSession() as session:\n                    async with session.post(\n                        self.base_url,\n                        headers=headers,\n                        json=payload,\n                        timeout=self.timeout\n                    ) as response:\n                        response.raise_for_status()\n                        result = await response.json()\n                        completion = result[\"choices\"][0][\"message\"][\"content\"]\n                        \n                        self.logger.debug(f\"Generated async completion: {completion[:50]}...\")\n                        return completion\n            except Exception as e:\n                self.logger.warning(f\"Async attempt {attempt+1} failed: {str(e)}\")\n                if attempt < self.max_retries - 1:\n                    await asyncio.sleep(2 ** attempt)  # Exponential backoff\n                else:\n                    self.logger.error(f\"All {self.max_retries} async attempts failed\")\n                    raise\n```\n\nEnsure the implementation handles API errors, rate limiting, and other common issues.",
      "testStrategy": "Create unit tests with mocked HTTP responses to verify both synchronous and asynchronous APIs. Test error handling by simulating various API errors and rate limits. Verify that retries work correctly with exponential backoff. Test timeout handling.",
      "priority": "high",
      "dependencies": [
        2
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 8,
      "title": "Implement PromptBuilder Component",
      "description": "Create a PromptBuilder component for constructing prompts for the LLM service",
      "details": "Implement a component for building prompts for the LLM service. Support templates, context management, and prompt optimization.\n\nExample implementation:\n```python\nimport logging\nfrom typing import Dict, Any, Optional\nfrom string import Template\n\nclass PromptBuilder:\n    def __init__(self, templates_path: Optional[str] = None, logger: Optional[logging.Logger] = None):\n        self.logger = logger or logging.getLogger(__name__)\n        self.templates: Dict[str, Template] = {}\n        \n        # Default templates\n        self.templates[\"analyze\"] = Template(\"\"\"Analyze the following pytest test failure and explain what might be causing it:\n            \nTest name: ${test_name}\nError message: ${error_message}\nTraceback:\n${traceback}\n\nProvide a detailed analysis of what's causing this failure:\"\"\")\n        \n        self.templates[\"suggest\"] = Template(\"\"\"Based on the following analysis of a pytest test failure, suggest a specific code fix:\n            \nTest name: ${test_name}\nError message: ${error_message}\nTraceback:\n${traceback}\n\nAnalysis:\n${analysis}\n\nProvide a specific code fix that would resolve this issue:\"\"\")\n        \n        # Load custom templates if provided\n        if templates_path:\n            self._load_templates(templates_path)\n    \n    def _load_templates(self, templates_path: str) -> None:\n        # Implementation to load templates from files\n        pass\n        \n    def build_prompt(self, template_name: str, context: Dict[str, Any]) -> str:\n        self.logger.debug(f\"Building prompt using template: {template_name}\")\n        \n        if template_name not in self.templates:\n            self.logger.error(f\"Template not found: {template_name}\")\n            raise KeyError(f\"Template not found: {template_name}\")\n            \n        try:\n            prompt = self.templates[template_name].substitute(context)\n            self.logger.debug(f\"Built prompt: {prompt[:50]}...\")\n            return prompt\n        except KeyError as e:\n            self.logger.error(f\"Missing context variable: {str(e)}\")\n            raise\n        except Exception as e:\n            self.logger.error(f\"Error building prompt: {str(e)}\")\n            raise\n            \n    def register_template(self, name: str, template_string: str) -> None:\n        self.logger.debug(f\"Registering template: {name}\")\n        self.templates[name] = Template(template_string)\n```\n\nEnsure the implementation supports loading templates from files and customizing templates at runtime.",
      "testStrategy": "Create unit tests to verify template substitution works correctly. Test error handling for missing templates and context variables. Verify that custom templates can be registered and used. Test loading templates from files.",
      "priority": "medium",
      "dependencies": [
        7
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 9,
      "title": "Implement ResponseParser Component",
      "description": "Create a ResponseParser component for parsing responses from the LLM service",
      "details": "Implement a component for parsing responses from the LLM service. Support extracting structured data, code blocks, and handling various response formats.\n\nExample implementation:\n```python\nimport logging\nimport re\nfrom typing import Dict, Any, List, Optional, Union\n\nclass ResponseParser:\n    def __init__(self, logger: Optional[logging.Logger] = None):\n        self.logger = logger or logging.getLogger(__name__)\n        \n    def extract_code_blocks(self, response: str) -> List[str]:\n        self.logger.debug(\"Extracting code blocks from response\")\n        \n        code_pattern = r\"```(?:python)?\\n([\\s\\S]+?)\\n```\"\n        code_blocks = re.findall(code_pattern, response)\n        \n        self.logger.debug(f\"Extracted {len(code_blocks)} code blocks\")\n        return code_blocks\n        \n    def extract_json(self, response: str) -> Optional[Dict[str, Any]]:\n        self.logger.debug(\"Extracting JSON from response\")\n        \n        json_pattern = r\"```json\\n([\\s\\S]+?)\\n```\"\n        json_match = re.search(json_pattern, response)\n        \n        if not json_match:\n            self.logger.warning(\"No JSON block found in response\")\n            return None\n            \n        import json\n        try:\n            json_str = json_match.group(1)\n            json_data = json.loads(json_str)\n            self.logger.debug(f\"Extracted JSON data with keys: {list(json_data.keys())}\")\n            return json_data\n        except json.JSONDecodeError as e:\n            self.logger.error(f\"Error parsing JSON: {str(e)}\")\n            return None\n            \n    def parse_structured_response(self, response: str, expected_sections: List[str]) -> Dict[str, str]:\n        self.logger.debug(f\"Parsing structured response with sections: {expected_sections}\")\n        \n        result = {}\n        current_section = None\n        current_content = []\n        \n        for line in response.split('\\n'):\n            # Check if line is a section header\n            for section in expected_sections:\n                if line.strip().lower() == section.lower() + ':' or line.strip().lower() == '## ' + section.lower():\n                    # Save previous section if it exists\n                    if current_section:\n                        result[current_section] = '\\n'.join(current_content).strip()\n                        current_content = []\n                    current_section = section\n                    break\n            else:\n                # If not a section header and we're in a section, add to content\n                if current_section:\n                    current_content.append(line)\n        \n        # Save the last section\n        if current_section and current_content:\n            result[current_section] = '\\n'.join(current_content).strip()\n            \n        # Check if all expected sections were found\n        missing_sections = [s for s in expected_sections if s not in result]\n        if missing_sections:\n            self.logger.warning(f\"Missing sections in response: {missing_sections}\")\n            \n        self.logger.debug(f\"Parsed {len(result)} sections from response\")\n        return result\n```\n\nEnsure the implementation can handle various response formats and extract structured data reliably.",
      "testStrategy": "Create unit tests with sample LLM responses to verify extraction of code blocks, JSON data, and structured sections. Test error handling for malformed responses. Verify that the parser correctly identifies missing sections and handles edge cases.",
      "priority": "medium",
      "dependencies": [
        7
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 10,
      "title": "Implement State Machine Base Class",
      "description": "Create a state machine base class for managing complex processes",
      "details": "Implement a base class for state machines to manage complex processes. Support state transitions, event handling, and error recovery.\n\nExample implementation:\n```python\nimport logging\nfrom typing import Dict, Any, List, Optional, Set, Callable, TypeVar, Generic\nfrom enum import Enum\n\nT = TypeVar('T', bound=Enum)\n\nclass StateMachine(Generic[T]):\n    def __init__(self, initial_state: T, logger: Optional[logging.Logger] = None):\n        self.current_state = initial_state\n        self.logger = logger or logging.getLogger(__name__)\n        self.transitions: Dict[T, Set[T]] = {}  # Valid state transitions\n        self.handlers: Dict[T, Callable[[], None]] = {}  # State handlers\n        self.error_handlers: Dict[T, Callable[[Exception], None]] = {}  # Error handlers\n        self.context: Dict[str, Any] = {}  # Shared context\n        \n    def add_transition(self, from_state: T, to_state: T) -> None:\n        self.logger.debug(f\"Adding transition: {from_state} -> {to_state}\")\n        \n        if from_state not in self.transitions:\n            self.transitions[from_state] = set()\n            \n        self.transitions[from_state].add(to_state)\n        \n    def add_handler(self, state: T, handler: Callable[[], None]) -> None:\n        self.logger.debug(f\"Adding handler for state: {state}\")\n        self.handlers[state] = handler\n        \n    def add_error_handler(self, state: T, handler: Callable[[Exception], None]) -> None:\n        self.logger.debug(f\"Adding error handler for state: {state}\")\n        self.error_handlers[state] = handler\n        \n    def transition_to(self, new_state: T) -> bool:\n        self.logger.info(f\"Attempting transition: {self.current_state} -> {new_state}\")\n        \n        # Check if transition is valid\n        if self.current_state in self.transitions and new_state in self.transitions[self.current_state]:\n            self.current_state = new_state\n            self.logger.info(f\"Transitioned to state: {new_state}\")\n            return True\n        else:\n            self.logger.warning(f\"Invalid transition: {self.current_state} -> {new_state}\")\n            return False\n            \n    def run(self) -> None:\n        self.logger.info(f\"Starting state machine in state: {self.current_state}\")\n        \n        while True:\n            if self.current_state not in self.handlers:\n                self.logger.warning(f\"No handler for state: {self.current_state}\")\n                break\n                \n            try:\n                self.handlers[self.current_state]()  # Execute handler for current state\n            except Exception as e:\n                self.logger.error(f\"Error in state {self.current_state}: {str(e)}\")\n                \n                # Execute error handler if available\n                if self.current_state in self.error_handlers:\n                    try:\n                        self.error_handlers[self.current_state](e)\n                    except Exception as err_e:\n                        self.logger.error(f\"Error in error handler: {str(err_e)}\")\n                        break\n                else:\n                    break\n                    \n            # If no more transitions, we're done\n            if self.current_state not in self.transitions or not self.transitions[self.current_state]:\n                self.logger.info(f\"Reached terminal state: {self.current_state}\")\n                break\n                \n        self.logger.info(f\"State machine execution completed in state: {self.current_state}\")\n```\n\nEnsure the implementation supports complex state transitions, error handling, and context sharing between states.",
      "testStrategy": "Create unit tests to verify state transitions, handler execution, and error handling. Test with simple state machines to verify the base functionality. Test error recovery mechanisms. Verify that invalid transitions are properly rejected.",
      "priority": "high",
      "dependencies": [],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 11,
      "title": "Implement Analyzer State Machine",
      "description": "Implement a state machine for the test analysis workflow",
      "details": "Implement a state machine for the test analysis workflow. Define states, transitions, and handlers for the complete process from extraction to applying fixes.\n\nExample implementation:\n```python\nimport logging\nfrom enum import Enum, auto\nfrom typing import Optional, List\nfrom .state_machine import StateMachine\nfrom .interfaces import Extractor, Analyzer, Suggester, Applier, TestFailure\n\nclass AnalysisState(Enum):\n    IDLE = auto()\n    EXTRACTING = auto()\n    ANALYZING = auto()\n    SUGGESTING = auto()\n    APPLYING = auto()\n    COMPLETED = auto()\n    ERROR = auto()\n\nclass AnalyzerStateMachine(StateMachine[AnalysisState]):\n    def __init__(self, extractor: Extractor, analyzer: Analyzer, \n                 suggester: Suggester, applier: Applier,\n                 logger: Optional[logging.Logger] = None):\n        super().__init__(AnalysisState.IDLE, logger)\n        \n        self.extractor = extractor\n        self.analyzer = analyzer\n        self.suggester = suggester\n        self.applier = applier\n        \n        # Define transitions\n        self.add_transition(AnalysisState.IDLE, AnalysisState.EXTRACTING)\n        self.add_transition(AnalysisState.EXTRACTING, AnalysisState.ANALYZING)\n        self.add_transition(AnalysisState.ANALYZING, AnalysisState.SUGGESTING)\n        self.add_transition(AnalysisState.SUGGESTING, AnalysisState.APPLYING)\n        self.add_transition(AnalysisState.APPLYING, AnalysisState.COMPLETED)\n        \n        # Error transitions\n        self.add_transition(AnalysisState.EXTRACTING, AnalysisState.ERROR)\n        self.add_transition(AnalysisState.ANALYZING, AnalysisState.ERROR)\n        self.add_transition(AnalysisState.SUGGESTING, AnalysisState.ERROR)\n        self.add_transition(AnalysisState.APPLYING, AnalysisState.ERROR)\n        \n        # Define handlers\n        self.add_handler(AnalysisState.IDLE, self._handle_idle)\n        self.add_handler(AnalysisState.EXTRACTING, self._handle_extracting)\n        self.add_handler(AnalysisState.ANALYZING, self._handle_analyzing)\n        self.add_handler(AnalysisState.SUGGESTING, self._handle_suggesting)\n        self.add_handler(AnalysisState.APPLYING, self._handle_applying)\n        self.add_handler(AnalysisState.COMPLETED, self._handle_completed)\n        self.add_handler(AnalysisState.ERROR, self._handle_error)\n        \n        # Define error handlers\n        self.add_error_handler(AnalysisState.EXTRACTING, self._handle_extraction_error)\n        self.add_error_handler(AnalysisState.ANALYZING, self._handle_analysis_error)\n        self.add_error_handler(AnalysisState.SUGGESTING, self._handle_suggestion_error)\n        self.add_error_handler(AnalysisState.APPLYING, self._handle_application_error)\n        \n    def start(self, test_output: str, file_path: str) -> None:\n        self.context[\"test_output\"] = test_output\n        self.context[\"file_path\"] = file_path\n        self.context[\"failures\"] = []\n        self.context[\"analyses\"] = []\n        self.context[\"suggestions\"] = []\n        self.context[\"applied\"] = []\n        \n        self.transition_to(AnalysisState.EXTRACTING)\n        self.run()\n        \n    def _handle_idle(self) -> None:\n        self.logger.info(\"State machine is idle\")\n        \n    def _handle_extracting(self) -> None:\n        self.logger.info(\"Extracting test failures\")\n        test_output = self.context[\"test_output\"]\n        failures = self.extractor.extract_failures(test_output)\n        self.context[\"failures\"] = failures\n        \n        if failures:\n            self.transition_to(AnalysisState.ANALYZING)\n        else:\n            self.logger.info(\"No failures found, skipping to completed\")\n            self.transition_to(AnalysisState.COMPLETED)\n            \n    def _handle_analyzing(self) -> None:\n        self.logger.info(\"Analyzing test failures\")\n        failures: List[TestFailure] = self.context[\"failures\"]\n        analyses = []\n        \n        for failure in failures:\n            analysis = self.analyzer.analyze_failure(failure)\n            analyses.append(analysis)\n            \n        self.context[\"analyses\"] = analyses\n        self.transition_to(AnalysisState.SUGGESTING)\n        \n    def _handle_suggesting(self) -> None:\n        self.logger.info(\"Suggesting fixes\")\n        failures: List[TestFailure] = self.context[\"failures\"]\n        analyses: List[str] = self.context[\"analyses\"]\n        suggestions = []\n        \n        for failure, analysis in zip(failures, analyses):\n            suggestion = self.suggester.suggest_fix(analysis, failure)\n            suggestions.append(suggestion)\n            \n        self.context[\"suggestions\"] = suggestions\n        self.transition_to(AnalysisState.APPLYING)\n        \n    def _handle_applying(self) -> None:\n        self.logger.info(\"Applying fixes\")\n        file_path = self.context[\"file_path\"]\n        suggestions: List[str] = self.context[\"suggestions\"]\n        applied = []\n        \n        for suggestion in suggestions:\n            success = self.applier.apply_fix(suggestion, file_path)\n            applied.append(success)\n            \n        self.context[\"applied\"] = applied\n        self.transition_to(AnalysisState.COMPLETED)\n        \n    def _handle_completed(self) -> None:\n        self.logger.info(\"Analysis workflow completed\")\n        failures = len(self.context.get(\"failures\", []))\n        applied = sum(self.context.get(\"applied\", []))\n        self.logger.info(f\"Processed {failures} failures, successfully applied {applied} fixes\")\n        \n    def _handle_error(self) -> None:\n        self.logger.error(f\"Analysis workflow failed: {self.context.get('error', 'Unknown error')}\")\n        \n    def _handle_extraction_error(self, e: Exception) -> None:\n        self.logger.error(f\"Extraction error: {str(e)}\")\n        self.context[\"error\"] = f\"Extraction error: {str(e)}\"\n        self.transition_to(AnalysisState.ERROR)\n        \n    def _handle_analysis_error(self, e: Exception) -> None:\n        self.logger.error(f\"Analysis error: {str(e)}\")\n        self.context[\"error\"] = f\"Analysis error: {str(e)}\"\n        self.transition_to(AnalysisState.ERROR)\n        \n    def _handle_suggestion_error(self, e: Exception) -> None:\n        self.logger.error(f\"Suggestion error: {str(e)}\")\n        self.context[\"error\"] = f\"Suggestion error: {str(e)}\"\n        self.transition_to(AnalysisState.ERROR)\n        \n    def _handle_application_error(self, e: Exception) -> None:\n        self.logger.error(f\"Application error: {str(e)}\")\n        self.context[\"error\"] = f\"Application error: {str(e)}\"\n        self.transition_to(AnalysisState.ERROR)\n```\n\nEnsure the implementation handles all states and transitions properly, with appropriate error handling.",
      "testStrategy": "Create unit tests to verify the state machine transitions through all states correctly. Mock the component dependencies to test each state handler in isolation. Test error handling by simulating failures in each state. Verify that the context is properly updated at each step.",
      "priority": "high",
      "dependencies": [
        3,
        4,
        5,
        6,
        10
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 12,
      "title": "Implement Error Handling Mechanism",
      "description": "Establish a consistent error handling mechanism throughout the codebase",
      "details": "Implement a consistent error handling mechanism. Create custom exception classes, context managers for resource handling, and standardized error reporting.\n\nExample implementation:\n```python\nimport logging\nfrom typing import Optional, Type, TypeVar, Generic, Callable, Any\nfrom contextlib import contextmanager\n\n# Custom exceptions\nclass PytestAnalyzerError(Exception):\n    \"\"\"Base exception for all pytest-analyzer errors\"\"\"\n    pass\n\nclass ExtractionError(PytestAnalyzerError):\n    \"\"\"Error during test failure extraction\"\"\"\n    pass\n\nclass AnalysisError(PytestAnalyzerError):\n    \"\"\"Error during test failure analysis\"\"\"\n    pass\n\nclass SuggestionError(PytestAnalyzerError):\n    \"\"\"Error during fix suggestion\"\"\"\n    pass\n\nclass ApplicationError(PytestAnalyzerError):\n    \"\"\"Error during fix application\"\"\"\n    pass\n\nclass LLMServiceError(PytestAnalyzerError):\n    \"\"\"Error during LLM service interaction\"\"\"\n    pass\n\n# Error context manager\nT = TypeVar('T')\n\nclass ErrorHandler(Generic[T]):\n    def __init__(self, logger: Optional[logging.Logger] = None):\n        self.logger = logger or logging.getLogger(__name__)\n        \n    @contextmanager\n    def handle_errors(self, operation: str, error_type: Type[PytestAnalyzerError] = PytestAnalyzerError):\n        try:\n            yield\n        except error_type as e:\n            self.logger.error(f\"{operation} failed: {str(e)}\")\n            raise\n        except Exception as e:\n            self.logger.error(f\"Unexpected error during {operation}: {str(e)}\")\n            raise error_type(f\"Unexpected error during {operation}: {str(e)}\") from e\n            \n    def retry(self, operation: str, max_retries: int = 3, \n              error_type: Type[PytestAnalyzerError] = PytestAnalyzerError,\n              should_retry: Callable[[Exception], bool] = lambda _: True):\n        def decorator(func):\n            def wrapper(*args, **kwargs):\n                last_exception = None\n                \n                for attempt in range(max_retries):\n                    try:\n                        return func(*args, **kwargs)\n                    except Exception as e:\n                        last_exception = e\n                        if not should_retry(e) or attempt >= max_retries - 1:\n                            break\n                            \n                        self.logger.warning(f\"{operation} attempt {attempt+1} failed: {str(e)}\")\n                        \n                # If we get here, all retries failed\n                self.logger.error(f\"{operation} failed after {max_retries} attempts\")\n                if isinstance(last_exception, PytestAnalyzerError):\n                    raise last_exception\n                else:\n                    raise error_type(f\"{operation} failed: {str(last_exception)}\") from last_exception\n                    \n            return wrapper\n        return decorator\n```\n\nEnsure the implementation provides consistent error handling and reporting throughout the codebase.",
      "testStrategy": "Create unit tests to verify error handling in various scenarios. Test the context manager with different types of exceptions. Test the retry decorator with both recoverable and non-recoverable errors. Verify that custom exceptions are properly raised and contain appropriate context information.",
      "priority": "medium",
      "dependencies": [],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 13,
      "title": "Implement Logging and Telemetry",
      "description": "Enhance logging with consistent levels and messages, and add telemetry for important operations",
      "details": "Implement a consistent logging and telemetry system. Create a logging configuration, telemetry events, and performance monitoring.\n\nExample implementation:\n```python\nimport logging\nimport time\nimport json\nfrom typing import Dict, Any, Optional, Callable\nfrom functools import wraps\n\nclass LoggingManager:\n    def __init__(self, log_level: int = logging.INFO, log_file: Optional[str] = None):\n        self.log_level = log_level\n        self.log_file = log_file\n        self._configure_logging()\n        \n    def _configure_logging(self) -> None:\n        root_logger = logging.getLogger()\n        root_logger.setLevel(self.log_level)\n        \n        # Clear existing handlers\n        for handler in root_logger.handlers[:]:  \n            root_logger.removeHandler(handler)\n            \n        # Console handler\n        console_handler = logging.StreamHandler()\n        console_handler.setLevel(self.log_level)\n        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n        console_handler.setFormatter(formatter)\n        root_logger.addHandler(console_handler)\n        \n        # File handler if specified\n        if self.log_file:\n            file_handler = logging.FileHandler(self.log_file)\n            file_handler.setLevel(self.log_level)\n            file_handler.setFormatter(formatter)\n            root_logger.addHandler(file_handler)\n            \n    def get_logger(self, name: str) -> logging.Logger:\n        return logging.getLogger(name)\n\nclass TelemetryManager:\n    def __init__(self, logger: Optional[logging.Logger] = None, \n                 telemetry_file: Optional[str] = None):\n        self.logger = logger or logging.getLogger(__name__)\n        self.telemetry_file = telemetry_file\n        \n    def record_event(self, event_type: str, data: Dict[str, Any]) -> None:\n        event = {\n            \"timestamp\": time.time(),\n            \"type\": event_type,\n            \"data\": data\n        }\n        \n        self.logger.debug(f\"Telemetry event: {event_type}\")\n        \n        if self.telemetry_file:\n            try:\n                with open(self.telemetry_file, 'a') as f:\n                    f.write(json.dumps(event) + '\\n')\n            except Exception as e:\n                self.logger.warning(f\"Failed to write telemetry event: {str(e)}\")\n                \n    def measure_performance(self, operation_name: str):\n        def decorator(func):\n            @wraps(func)\n            def wrapper(*args, **kwargs):\n                start_time = time.time()\n                result = func(*args, **kwargs)\n                end_time = time.time()\n                duration = end_time - start_time\n                \n                self.record_event(\"performance\", {\n                    \"operation\": operation_name,\n                    \"duration\": duration\n                })\n                \n                return result\n            return wrapper\n        return decorator\n```\n\nEnsure the implementation provides consistent logging and telemetry throughout the codebase.",
      "testStrategy": "Create unit tests to verify logging configuration and telemetry recording. Test the performance measurement decorator with various functions. Verify that telemetry events are properly recorded to the specified file. Test error handling when the telemetry file is not writable.",
      "priority": "medium",
      "dependencies": [],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 14,
      "title": "Implement Configuration Management",
      "description": "Create a configuration management system for the application",
      "details": "Implement a configuration management system. Support loading configuration from files, environment variables, and command-line arguments.\n\nExample implementation:\n```python\nimport os\nimport json\nimport logging\nfrom typing import Dict, Any, Optional\nfrom dataclasses import dataclass, field\n\n@dataclass\nclass LLMConfig:\n    api_key: str = \"\"\n    model: str = \"gpt-4\"\n    max_retries: int = 3\n    timeout: int = 30\n\n@dataclass\nclass LoggingConfig:\n    log_level: str = \"INFO\"\n    log_file: Optional[str] = None\n    telemetry_file: Optional[str] = None\n\n@dataclass\nclass AppConfig:\n    llm: LLMConfig = field(default_factory=LLMConfig)\n    logging: LoggingConfig = field(default_factory=LoggingConfig)\n    templates_path: Optional[str] = None\n\nclass ConfigManager:\n    def __init__(self, logger: Optional[logging.Logger] = None):\n        self.logger = logger or logging.getLogger(__name__)\n        self.config = AppConfig()\n        \n    def load_from_file(self, file_path: str) -> None:\n        self.logger.info(f\"Loading configuration from file: {file_path}\")\n        \n        if not os.path.exists(file_path):\n            self.logger.warning(f\"Configuration file not found: {file_path}\")\n            return\n            \n        try:\n            with open(file_path, 'r') as f:\n                config_data = json.load(f)\n                \n            # Update LLM config\n            if \"llm\" in config_data:\n                llm_config = config_data[\"llm\"]\n                if \"api_key\" in llm_config:\n                    self.config.llm.api_key = llm_config[\"api_key\"]\n                if \"model\" in llm_config:\n                    self.config.llm.model = llm_config[\"model\"]\n                if \"max_retries\" in llm_config:\n                    self.config.llm.max_retries = llm_config[\"max_retries\"]\n                if \"timeout\" in llm_config:\n                    self.config.llm.timeout = llm_config[\"timeout\"]\n                    \n            # Update logging config\n            if \"logging\" in config_data:\n                logging_config = config_data[\"logging\"]\n                if \"log_level\" in logging_config:\n                    self.config.logging.log_level = logging_config[\"log_level\"]\n                if \"log_file\" in logging_config:\n                    self.config.logging.log_file = logging_config[\"log_file\"]\n                if \"telemetry_file\" in logging_config:\n                    self.config.logging.telemetry_file = logging_config[\"telemetry_file\"]\n                    \n            # Update other config\n            if \"templates_path\" in config_data:\n                self.config.templates_path = config_data[\"templates_path\"]\n                \n            self.logger.info(\"Configuration loaded successfully\")\n        except Exception as e:\n            self.logger.error(f\"Error loading configuration: {str(e)}\")\n            \n    def load_from_env(self) -> None:\n        self.logger.info(\"Loading configuration from environment variables\")\n        \n        # LLM config\n        if \"PYTEST_ANALYZER_API_KEY\" in os.environ:\n            self.config.llm.api_key = os.environ[\"PYTEST_ANALYZER_API_KEY\"]\n        if \"PYTEST_ANALYZER_MODEL\" in os.environ:\n            self.config.llm.model = os.environ[\"PYTEST_ANALYZER_MODEL\"]\n        if \"PYTEST_ANALYZER_MAX_RETRIES\" in os.environ:\n            try:\n                self.config.llm.max_retries = int(os.environ[\"PYTEST_ANALYZER_MAX_RETRIES\"])\n            except ValueError:\n                self.logger.warning(\"Invalid value for PYTEST_ANALYZER_MAX_RETRIES\")\n        if \"PYTEST_ANALYZER_TIMEOUT\" in os.environ:\n            try:\n                self.config.llm.timeout = int(os.environ[\"PYTEST_ANALYZER_TIMEOUT\"])\n            except ValueError:\n                self.logger.warning(\"Invalid value for PYTEST_ANALYZER_TIMEOUT\")\n                \n        # Logging config\n        if \"PYTEST_ANALYZER_LOG_LEVEL\" in os.environ:\n            self.config.logging.log_level = os.environ[\"PYTEST_ANALYZER_LOG_LEVEL\"]\n        if \"PYTEST_ANALYZER_LOG_FILE\" in os.environ:\n            self.config.logging.log_file = os.environ[\"PYTEST_ANALYZER_LOG_FILE\"]\n        if \"PYTEST_ANALYZER_TELEMETRY_FILE\" in os.environ:\n            self.config.logging.telemetry_file = os.environ[\"PYTEST_ANALYZER_TELEMETRY_FILE\"]\n            \n        # Other config\n        if \"PYTEST_ANALYZER_TEMPLATES_PATH\" in os.environ:\n            self.config.templates_path = os.environ[\"PYTEST_ANALYZER_TEMPLATES_PATH\"]\n            \n        self.logger.info(\"Environment configuration loaded successfully\")\n        \n    def get_config(self) -> AppConfig:\n        return self.config\n```\n\nEnsure the implementation supports loading configuration from multiple sources with appropriate precedence.",
      "testStrategy": "Create unit tests to verify configuration loading from files and environment variables. Test with various configuration formats and values. Verify that configuration precedence works correctly. Test error handling for invalid configuration files and values.",
      "priority": "medium",
      "dependencies": [],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 15,
      "title": "Create Backward Compatibility Layer",
      "description": "Ensure backward compatibility with existing code",
      "details": "Implement a backward compatibility layer to ensure existing code continues to work with the refactored components. Create adapter classes and facade patterns as needed.\n\nExample implementation:\n```python\nimport logging\nfrom typing import Optional, List, Dict, Any\n\n# Import new components\nfrom .di_container import DIContainer\nfrom .interfaces import Extractor, Analyzer, Suggester, Applier, LLMService, TestFailure\nfrom .state_machine import AnalyzerStateMachine, AnalysisState\n\n# Legacy class to maintain backward compatibility\nclass PytestAnalyzer:\n    def __init__(self, api_key: Optional[str] = None, model: str = \"gpt-4\", logger: Optional[logging.Logger] = None):\n        self.logger = logger or logging.getLogger(__name__)\n        self.logger.info(\"Initializing PytestAnalyzer with backward compatibility\")\n        \n        # Set up DI container\n        self.container = DIContainer()\n        \n        # Configure and register services\n        from .config_manager import ConfigManager\n        config_manager = ConfigManager(logger=self.logger)\n        config_manager.load_from_env()\n        \n        # Override with constructor parameters if provided\n        config = config_manager.get_config()\n        if api_key:\n            config.llm.api_key = api_key\n        if model:\n            config.llm.model = model\n            \n        # Register LLM service\n        from .llm_service import OpenAIService\n        llm_service = OpenAIService(\n            api_key=config.llm.api_key,\n            model=config.llm.model,\n            max_retries=config.llm.max_retries,\n            timeout=config.llm.timeout,\n            logger=self.logger\n        )\n        self.container.register(LLMService, llm_service)\n        \n        # Register other components\n        from .extractor import PytestExtractor\n        from .analyzer import LLMBasedAnalyzer\n        from .suggester import LLMBasedSuggester\n        from .applier import FileSystemApplier\n        \n        self.container.register(Extractor, PytestExtractor(logger=self.logger))\n        self.container.register(Analyzer, LLMBasedAnalyzer(llm_service, logger=self.logger))\n        self.container.register(Suggester, LLMBasedSuggester(llm_service, logger=self.logger))\n        self.container.register(Applier, FileSystemApplier(logger=self.logger))\n        \n    def analyze_test_failures(self, test_output: str, file_path: str) -> Dict[str, Any]:\n        \"\"\"Legacy method to analyze test failures and suggest fixes\"\"\"\n        self.logger.info(\"Analyzing test failures using state machine\")\n        \n        # Create and run state machine\n        state_machine = AnalyzerStateMachine(\n            extractor=self.container.resolve(Extractor),\n            analyzer=self.container.resolve(Analyzer),\n            suggester=self.container.resolve(Suggester),\n            applier=self.container.resolve(Applier),\n            logger=self.logger\n        )\n        \n        state_machine.start(test_output, file_path)\n        \n        # Convert state machine results to legacy format\n        result = {\n            \"failures\": [],\n            \"analyses\": [],\n            \"suggestions\": [],\n            \"applied\": []\n        }\n        \n        if \"failures\" in state_machine.context:\n            result[\"failures\"] = [\n                {\"test_name\": f.test_name, \"error_message\": f.error_message, \"traceback\": f.traceback}\n                for f in state_machine.context[\"failures\"]\n            ]\n            \n        if \"analyses\" in state_machine.context:\n            result[\"analyses\"] = state_machine.context[\"analyses\"]\n            \n        if \"suggestions\" in state_machine.context:\n            result[\"suggestions\"] = state_machine.context[\"suggestions\"]\n            \n        if \"applied\" in state_machine.context:\n            result[\"applied\"] = state_machine.context[\"applied\"]\n            \n        if state_machine.current_state == AnalysisState.ERROR and \"error\" in state_machine.context:\n            result[\"error\"] = state_machine.context[\"error\"]\n            \n        return result\n        \n    def analyze_and_fix(self, test_output: str, file_path: str) -> bool:\n        \"\"\"Legacy method to analyze and automatically fix test failures\"\"\"\n        result = self.analyze_test_failures(test_output, file_path)\n        return all(result.get(\"applied\", [False]))\n```\n\nEnsure the implementation maintains the same API and behavior as the original code while using the new architecture internally.",
      "testStrategy": "Create integration tests that verify the backward compatibility layer works with existing code. Test with the same inputs and expected outputs as the original code. Verify that the new implementation produces the same results as the original. Test error handling and edge cases to ensure consistent behavior.",
      "priority": "high",
      "dependencies": [
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        11
      ],
      "status": "pending",
      "subtasks": []
    }
  ]
}
