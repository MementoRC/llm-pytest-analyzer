# Task ID: 7
# Title: Implement LLM Service
# Status: pending
# Dependencies: 2
# Priority: high
# Description: Create a proper LLM service with configuration and resource management injected
# Details:
Implement a service for interacting with language models. Support both synchronous and asynchronous APIs. Implement proper error handling, retries, and resource management.

Example implementation:
```python
import logging
import asyncio
import time
from typing import Optional, Dict, Any
import aiohttp
import requests
from .interfaces import LLMService

class OpenAIService(LLMService):
    def __init__(self, api_key: str, model: str = "gpt-4",
                 max_retries: int = 3, timeout: int = 30,
                 logger: Optional[logging.Logger] = None):
        self.api_key = api_key
        self.model = model
        self.max_retries = max_retries
        self.timeout = timeout
        self.logger = logger or logging.getLogger(__name__)
        self.base_url = "https://api.openai.com/v1/chat/completions"

    def _prepare_payload(self, prompt: str) -> Dict[str, Any]:
        return {
            "model": self.model,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.7
        }

    def generate_completion(self, prompt: str) -> str:
        self.logger.debug(f"Generating completion for prompt: {prompt[:50]}...")

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        payload = self._prepare_payload(prompt)

        for attempt in range(self.max_retries):
            try:
                response = requests.post(
                    self.base_url,
                    headers=headers,
                    json=payload,
                    timeout=self.timeout
                )

                response.raise_for_status()
                result = response.json()
                completion = result["choices"][0]["message"]["content"]

                self.logger.debug(f"Generated completion: {completion[:50]}...")
                return completion
            except Exception as e:
                self.logger.warning(f"Attempt {attempt+1} failed: {str(e)}")
                if attempt < self.max_retries - 1:
                    time.sleep(2 ** attempt)  # Exponential backoff
                else:
                    self.logger.error(f"All {self.max_retries} attempts failed")
                    raise

    async def generate_completion_async(self, prompt: str) -> str:
        self.logger.debug(f"Generating async completion for prompt: {prompt[:50]}...")

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        payload = self._prepare_payload(prompt)

        for attempt in range(self.max_retries):
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        self.base_url,
                        headers=headers,
                        json=payload,
                        timeout=self.timeout
                    ) as response:
                        response.raise_for_status()
                        result = await response.json()
                        completion = result["choices"][0]["message"]["content"]

                        self.logger.debug(f"Generated async completion: {completion[:50]}...")
                        return completion
            except Exception as e:
                self.logger.warning(f"Async attempt {attempt+1} failed: {str(e)}")
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(2 ** attempt)  # Exponential backoff
                else:
                    self.logger.error(f"All {self.max_retries} async attempts failed")
                    raise
```

Ensure the implementation handles API errors, rate limiting, and other common issues.

# Test Strategy:
Create unit tests with mocked HTTP responses to verify both synchronous and asynchronous APIs. Test error handling by simulating various API errors and rate limits. Verify that retries work correctly with exponential backoff. Test timeout handling.
