# Task ID: 10
# Title: Implement Core Analysis Tool: suggest_fixes
# Status: pending
# Dependencies: 6, 7
# Priority: high
# Description: Implement the MCP tool for generating fix suggestions from raw pytest output.
# Details:
In `src/pytest_analyzer/mcp/tools/analysis.py`, implement:
1. `suggest_fixes` tool function
2. Input validation for pytest output string
3. Size limit enforcement
4. Integration with MCPAnalyzerFacade
5. Format detection and parsing

Implementation should:
- Accept raw pytest output string
- Detect output format (JSON, XML, text)
- Validate input size limits
- Parse and analyze the output
- Return structured fix suggestions with code changes
- Include confidence scores and explanations

Example implementation:
```python
from pytest_analyzer.mcp.schemas import SuggestFixesRequest, SuggestFixesResponse

async def suggest_fixes(request: SuggestFixesRequest, facade: MCPAnalyzerFacade) -> SuggestFixesResponse:
    # Validate input size
    if len(request.pytest_output) > 1_000_000:  # 1MB limit
        raise ValueError(f"Input too large: {len(request.pytest_output)} bytes (max 1MB)")

    # Detect format
    output_format = "text"  # Default
    if request.pytest_output.startswith("{"):
        output_format = "json"
    elif request.pytest_output.startswith("<?xml"):
        output_format = "xml"

    # Delegate to facade
    result = await facade.suggest_fixes_from_string(
        request.pytest_output,
        format=output_format
    )

    return SuggestFixesResponse(
        suggestions=result.suggestions,
        execution_time_ms=result.execution_time_ms,
        detected_format=output_format
    )
```

Register this tool with the MCP server during initialization.

# Test Strategy:
Unit test with mock facade. Test format detection with various input types. Verify size limit enforcement. Test with realistic pytest outputs of different formats. Measure performance with large inputs. Integration test with actual facade implementation.
