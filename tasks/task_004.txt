# Task ID: 4
# Title: Implement LLM Service Interface and Base Implementation
# Status: done
# Dependencies: 2, 3
# Priority: high
# Description: Create a properly abstracted LLM service with both synchronous and asynchronous APIs.
# Details:
Implement the LLM service with the following components:

```python
from typing import Dict, Any, Optional, Protocol
import aiohttp
import requests
from contextlib import contextmanager

class LLMConfig(Protocol):
    api_key: str
    model_name: str
    temperature: float
    max_tokens: int
    # Other configuration parameters

class LLMService:
    def __init__(self, config: LLMConfig):
        self.config = config
        
    @contextmanager
    def _error_context(self, operation: str):
        try:
            yield
        except Exception as e:
            # Log the error with context
            raise LLMServiceError(f"Error during {operation}: {str(e)}") from e
    
    def generate(self, prompt: str, **kwargs) -> str:
        with self._error_context("synchronous generation"):
            # Implementation for synchronous API call
            params = {**self.config.__dict__, **kwargs}
            response = requests.post(
                "https://api.openai.com/v1/completions",
                headers={"Authorization": f"Bearer {self.config.api_key}"},
                json={"prompt": prompt, **params}
            )
            response.raise_for_status()
            return response.json()["choices"][0]["text"]
    
    async def generate_async(self, prompt: str, **kwargs) -> str:
        with self._error_context("asynchronous generation"):
            # Implementation for asynchronous API call
            params = {**self.config.__dict__, **kwargs}
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    "https://api.openai.com/v1/completions",
                    headers={"Authorization": f"Bearer {self.config.api_key}"},
                    json={"prompt": prompt, **params}
                ) as response:
                    if response.status != 200:
                        text = await response.text()
                        raise LLMServiceError(f"API error: {response.status}, {text}")
                    data = await response.json()
                    return data["choices"][0]["text"]

class LLMServiceError(Exception):
    pass
```

Ensure the implementation is configurable, handles errors properly, and provides both synchronous and asynchronous APIs.

# Test Strategy:
Create unit tests with mocked HTTP responses to test both successful and error scenarios. Test both synchronous and asynchronous APIs. Verify error handling and context managers work as expected.
